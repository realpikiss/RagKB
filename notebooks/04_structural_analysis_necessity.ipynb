{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5760b150",
   "metadata": {},
   "source": [
    "# 🏗️ Phase 4: Structural Analysis Necessity\n",
    "\n",
    "**Objective:** Determine if you actually need CFG/PDG or if AST suffices\n",
    "\n",
    "**Key Deliverables:**\n",
    "- AST-only classification effectiveness analysis\n",
    "- Control flow complexity assessment\n",
    "- Data dependency requirements evaluation\n",
    "- Architecture simplification opportunities\n",
    "- Evidence-based component selection\n",
    "\n",
    "**Building on Previous Phases:**\n",
    "- Phase 1: Dataset characteristics understood\n",
    "- Phase 2: Processing performance validated (0.004s avg)\n",
    "- Phase 3: Context-based approach proven superior\n",
    "- Now: Determine optimal architecture complexity\n",
    "\n",
    "**Scientific Question:** \n",
    "*Can AST patterns alone capture the vulnerability context, or do we need full CFG/PDG analysis?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656e525c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported processing modules\n",
      "🏗️ Phase 4 Setup complete!\n",
      "Project root: /Users/vernetemmanueladjobi/Desktop/KB_/vulnerability-kb\n",
      "Results directory: ../results\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Any\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project source to path\n",
    "project_root = Path('../')\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "# Import processing modules\n",
    "try:\n",
    "    from extract_ast import extract_ast_patterns\n",
    "    from build_cfg import build_simple_cfg\n",
    "    from build_pdg import build_simple_pdg\n",
    "    print(\"✅ Successfully imported processing modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "\n",
    "# Set up paths\n",
    "data_dir = project_root / 'data'\n",
    "raw_dir = data_dir / 'raw'\n",
    "results_dir = project_root / 'results'\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "print(\"🏗️ Phase 4 Setup complete!\")\n",
    "print(f\"Project root: {project_root.resolve()}\")\n",
    "print(f\"Results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13169721",
   "metadata": {},
   "source": [
    "## 4.1 Load Previous Results and Vulnerability Data\n",
    "\n",
    "Load insights from Phases 1-3 to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d0953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loading previous phase results...\n",
      "✅ Phase 1: 1217 CVEs\n",
      "✅ Phase 2: 2317 performance samples\n",
      "✅ Phase 3: Context analysis configuration loaded\n",
      "📂 Loading ALL vulnerability samples for structural analysis...\n",
      "✅ Loaded 2317 vulnerability samples (100% of dataset)\n"
     ]
    }
   ],
   "source": [
    "# Load previous phase results\n",
    "print(\"📂 Loading previous phase results...\")\n",
    "\n",
    "# Load Phase 1 summary\n",
    "try:\n",
    "    with open(results_dir / 'vulrag_summary_report.json', 'r') as f:\n",
    "        phase1_summary = json.load(f)\n",
    "    print(f\"✅ Phase 1: {phase1_summary['analysis_metadata']['total_cves']} CVEs\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Phase 1 summary not found\")\n",
    "    raise\n",
    "\n",
    "# Load Phase 2 performance results\n",
    "try:\n",
    "    phase2_df = pd.read_csv(results_dir / 'performance_analysis_results.csv')\n",
    "    print(f\"✅ Phase 2: {len(phase2_df)} performance samples\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Phase 2 results not found\")\n",
    "    raise\n",
    "\n",
    "# Load Phase 3 context analysis\n",
    "try:\n",
    "    with open(results_dir / 'context_based_analysis_config.json', 'r') as f:\n",
    "        phase3_config = json.load(f)\n",
    "    print(f\"✅ Phase 3: Context analysis configuration loaded\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Phase 3 configuration not found\")\n",
    "    raise\n",
    "\n",
    "# Modifier la fonction pour analyser TOUT le dataset\n",
    "def load_analysis_samples(sample_limit=None):  # None = tous les échantillons\n",
    "    \"\"\"Load ALL vulnerability samples for structural analysis\"\"\"\n",
    "    print(f\"📂 Loading ALL vulnerability samples for structural analysis...\")\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    # Get CWE files\n",
    "    cwe_files = list(raw_dir.glob(\"*.json\"))\n",
    "    \n",
    "    for cwe_file in sorted(cwe_files):\n",
    "        try:\n",
    "            with open(cwe_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            cwe = cwe_file.stem.split('_')[1]\n",
    "            \n",
    "            for cve_id, instances in data.items():\n",
    "                for idx, instance in enumerate(instances):\n",
    "                    code_before = instance.get('code_before_change', '')\n",
    "                    if code_before and len(code_before.strip()) > 20:\n",
    "                        samples.append({\n",
    "                            'cwe': cwe,\n",
    "                            'cve_id': cve_id,\n",
    "                            'instance_idx': idx,\n",
    "                            'code': code_before,\n",
    "                            'lines': len(code_before.split('\\n')),\n",
    "                            'chars': len(code_before)\n",
    "                        })\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {cwe_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"✅ Loaded {len(samples)} vulnerability samples (100% of dataset)\")\n",
    "    return samples\n",
    "\n",
    "# Load samples\n",
    "analysis_samples = load_analysis_samples(sample_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315db97",
   "metadata": {},
   "source": [
    "## 4.2 AST-Only Classification Effectiveness\n",
    "\n",
    "Test if AST patterns alone can detect the vulnerabilities found in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2437cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ast_only_effectiveness(samples):\n",
    "    \"\"\"Analyze how well AST patterns alone can detect vulnerabilities\"\"\"\n",
    "    print(\" ANALYZING AST-ONLY CLASSIFICATION EFFECTIVENESS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    ast_results = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"📋 Processing sample {i+1}/{len(samples)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Extract AST patterns - CORRECTION: use timeout_seconds\n",
    "            ast_result = extract_ast_patterns(sample['code'], timeout_seconds=10)\n",
    "            \n",
    "            if ast_result.get('success'):\n",
    "                patterns = ast_result.get('patterns', {})\n",
    "                \n",
    "                # Analyze what AST can detect\n",
    "                ast_detection = {\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'functions_detected': len(patterns.get('functions', [])),\n",
    "                    'calls_detected': len(patterns.get('calls', [])),\n",
    "                    'variables_detected': len(patterns.get('variables', [])),\n",
    "                    'pointers_detected': len(patterns.get('pointers', [])),\n",
    "                    'arrays_detected': len(patterns.get('arrays', [])),\n",
    "                    'conditionals_detected': len(patterns.get('conditions', [])),\n",
    "                    'loops_detected': len(patterns.get('loops', [])),\n",
    "                    'ast_success': True\n",
    "                }\n",
    "                \n",
    "                # Check for vulnerability indicators in AST\n",
    "                vulnerability_indicators = []\n",
    "                \n",
    "                # Buffer-related indicators\n",
    "                if any('buffer' in var.get('name', '').lower() for var in patterns.get('variables', [])):\n",
    "                    vulnerability_indicators.append('buffer_variables')\n",
    "                \n",
    "                # Unsafe function calls\n",
    "                unsafe_functions = ['strcpy', 'strcat', 'sprintf', 'gets', 'scanf', 'memcpy']\n",
    "                unsafe_calls = [call for call in patterns.get('calls', []) \n",
    "                              if call.get('function', '') in unsafe_functions]\n",
    "                if unsafe_calls:\n",
    "                    vulnerability_indicators.append('unsafe_functions')\n",
    "                \n",
    "                # Pointer operations\n",
    "                if patterns.get('pointers'):\n",
    "                    vulnerability_indicators.append('pointer_operations')\n",
    "                \n",
    "                # Array operations\n",
    "                if patterns.get('arrays'):\n",
    "                    vulnerability_indicators.append('array_operations')\n",
    "                \n",
    "                ast_detection['vulnerability_indicators'] = vulnerability_indicators\n",
    "                ast_detection['indicator_count'] = len(vulnerability_indicators)\n",
    "                \n",
    "            else:\n",
    "                ast_detection = {\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'ast_success': False,\n",
    "                    'vulnerability_indicators': [],\n",
    "                    'indicator_count': 0\n",
    "                }\n",
    "            \n",
    "            ast_results.append(ast_detection)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error analyzing sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return ast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9cc1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing AST-only vulnerability detection...\n",
      " ANALYZING AST-ONLY CLASSIFICATION EFFECTIVENESS\n",
      "============================================================\n",
      "📋 Processing sample 1/2317...\n",
      "📋 Processing sample 21/2317...\n",
      "📋 Processing sample 41/2317...\n",
      "📋 Processing sample 61/2317...\n",
      "📋 Processing sample 81/2317...\n",
      "📋 Processing sample 101/2317...\n",
      "📋 Processing sample 121/2317...\n",
      "📋 Processing sample 141/2317...\n",
      "📋 Processing sample 161/2317...\n",
      "📋 Processing sample 181/2317...\n",
      "📋 Processing sample 201/2317...\n",
      "📋 Processing sample 221/2317...\n",
      "📋 Processing sample 241/2317...\n",
      "📋 Processing sample 261/2317...\n",
      "📋 Processing sample 281/2317...\n",
      "📋 Processing sample 301/2317...\n",
      "📋 Processing sample 321/2317...\n",
      "📋 Processing sample 341/2317...\n",
      "📋 Processing sample 361/2317...\n",
      "📋 Processing sample 381/2317...\n",
      "📋 Processing sample 401/2317...\n",
      "📋 Processing sample 421/2317...\n",
      "📋 Processing sample 441/2317...\n",
      "📋 Processing sample 461/2317...\n",
      "📋 Processing sample 481/2317...\n",
      "📋 Processing sample 501/2317...\n",
      "📋 Processing sample 521/2317...\n",
      "📋 Processing sample 541/2317...\n",
      "📋 Processing sample 561/2317...\n",
      "📋 Processing sample 581/2317...\n",
      "📋 Processing sample 601/2317...\n",
      "📋 Processing sample 621/2317...\n",
      "📋 Processing sample 641/2317...\n",
      "📋 Processing sample 661/2317...\n",
      "📋 Processing sample 681/2317...\n",
      "📋 Processing sample 701/2317...\n",
      "📋 Processing sample 721/2317...\n",
      "📋 Processing sample 741/2317...\n",
      "📋 Processing sample 761/2317...\n",
      "📋 Processing sample 781/2317...\n",
      "📋 Processing sample 801/2317...\n",
      "📋 Processing sample 821/2317...\n",
      "📋 Processing sample 841/2317...\n",
      "📋 Processing sample 861/2317...\n",
      "📋 Processing sample 881/2317...\n",
      "📋 Processing sample 901/2317...\n",
      "📋 Processing sample 921/2317...\n",
      "📋 Processing sample 941/2317...\n",
      "📋 Processing sample 961/2317...\n",
      "📋 Processing sample 981/2317...\n",
      "📋 Processing sample 1001/2317...\n",
      "📋 Processing sample 1021/2317...\n",
      "📋 Processing sample 1041/2317...\n",
      "📋 Processing sample 1061/2317...\n",
      "📋 Processing sample 1081/2317...\n",
      "📋 Processing sample 1101/2317...\n",
      "📋 Processing sample 1121/2317...\n",
      "📋 Processing sample 1141/2317...\n",
      "📋 Processing sample 1161/2317...\n",
      "📋 Processing sample 1181/2317...\n",
      "📋 Processing sample 1201/2317...\n",
      "📋 Processing sample 1221/2317...\n",
      "📋 Processing sample 1241/2317...\n",
      "📋 Processing sample 1261/2317...\n",
      "📋 Processing sample 1281/2317...\n",
      "📋 Processing sample 1301/2317...\n",
      "📋 Processing sample 1321/2317...\n",
      "📋 Processing sample 1341/2317...\n",
      "📋 Processing sample 1361/2317...\n",
      "📋 Processing sample 1381/2317...\n",
      "📋 Processing sample 1401/2317...\n",
      "📋 Processing sample 1421/2317...\n",
      "📋 Processing sample 1441/2317...\n",
      "📋 Processing sample 1461/2317...\n",
      "📋 Processing sample 1481/2317...\n",
      "📋 Processing sample 1501/2317...\n",
      "📋 Processing sample 1521/2317...\n",
      "📋 Processing sample 1541/2317...\n",
      "📋 Processing sample 1561/2317...\n",
      "📋 Processing sample 1581/2317...\n",
      "📋 Processing sample 1601/2317...\n",
      "📋 Processing sample 1621/2317...\n",
      "📋 Processing sample 1641/2317...\n",
      "📋 Processing sample 1661/2317...\n",
      "📋 Processing sample 1681/2317...\n",
      "📋 Processing sample 1701/2317...\n",
      "📋 Processing sample 1721/2317...\n",
      "📋 Processing sample 1741/2317...\n",
      "📋 Processing sample 1761/2317...\n",
      "📋 Processing sample 1781/2317...\n",
      "📋 Processing sample 1801/2317...\n",
      "📋 Processing sample 1821/2317...\n",
      "📋 Processing sample 1841/2317...\n",
      "📋 Processing sample 1861/2317...\n",
      "📋 Processing sample 1881/2317...\n",
      "📋 Processing sample 1901/2317...\n",
      "📋 Processing sample 1921/2317...\n",
      "📋 Processing sample 1941/2317...\n",
      "📋 Processing sample 1961/2317...\n",
      "📋 Processing sample 1981/2317...\n",
      "📋 Processing sample 2001/2317...\n",
      "📋 Processing sample 2021/2317...\n",
      "📋 Processing sample 2041/2317...\n",
      "📋 Processing sample 2061/2317...\n",
      "📋 Processing sample 2081/2317...\n",
      "📋 Processing sample 2101/2317...\n",
      "📋 Processing sample 2121/2317...\n",
      "📋 Processing sample 2141/2317...\n",
      "📋 Processing sample 2161/2317...\n",
      "📋 Processing sample 2181/2317...\n",
      "📋 Processing sample 2201/2317...\n",
      "📋 Processing sample 2221/2317...\n",
      "📋 Processing sample 2241/2317...\n",
      "📋 Processing sample 2261/2317...\n",
      "📋 Processing sample 2281/2317...\n",
      "📋 Processing sample 2301/2317...\n",
      "\n",
      "📊 AST-ONLY ANALYSIS RESULTS:\n",
      "   • Total samples: 2317\n",
      "   • AST extraction success: 2317/2317 (100.0%)\n",
      "   • Vulnerability indicators detected: 2249/2317 (97.1%)\n",
      "   • Most common indicators:\n",
      "      • pointer_operations: 2238/2317 (96.6%)\n",
      "      • array_operations: 725/2317 (31.3%)\n",
      "      • unsafe_functions: 226/2317 (9.8%)\n",
      "\n",
      " AST DETECTION BY CWE TYPE:\n",
      "   • CWE-119: 166/173 (96.0%)\n",
      "   • CWE-125: 138/140 (98.6%)\n",
      "   • CWE-200: 141/153 (92.2%)\n",
      "   • CWE-20: 178/182 (97.8%)\n",
      "   • CWE-264: 116/120 (96.7%)\n",
      "   • CWE-362: 314/320 (98.1%)\n",
      "   • CWE-401: 97/101 (96.0%)\n",
      "   • CWE-416: 647/660 (98.0%)\n",
      "   • CWE-476: 269/281 (95.7%)\n",
      "   • CWE-787: 183/187 (97.9%)\n",
      "\n",
      "💾 AST-only analysis saved to: ../results/ast_only_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Analyze AST-only effectiveness\n",
    "print(\" Testing AST-only vulnerability detection...\")\n",
    "ast_analysis = analyze_ast_only_effectiveness(analysis_samples)\n",
    "\n",
    "# Analyze results\n",
    "if ast_analysis:\n",
    "    ast_df = pd.DataFrame(ast_analysis)\n",
    "    successful_ast = ast_df[ast_df['ast_success']]\n",
    "    \n",
    "    print(f\"\\n📊 AST-ONLY ANALYSIS RESULTS:\")\n",
    "    print(f\"   • Total samples: {len(ast_df)}\")\n",
    "    print(f\"   • AST extraction success: {len(successful_ast)}/{len(ast_df)} ({len(successful_ast)/len(ast_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_ast) > 0:\n",
    "        # Vulnerability detection capability\n",
    "        samples_with_indicators = successful_ast[successful_ast['indicator_count'] > 0]\n",
    "        detection_rate = len(samples_with_indicators) / len(successful_ast) * 100\n",
    "        \n",
    "        print(f\"   • Vulnerability indicators detected: {len(samples_with_indicators)}/{len(successful_ast)} ({detection_rate:.1f}%)\")\n",
    "        \n",
    "        # Most common indicators\n",
    "        all_indicators = []\n",
    "        for indicators in successful_ast['vulnerability_indicators']:\n",
    "            all_indicators.extend(indicators)\n",
    "        \n",
    "        indicator_counter = Counter(all_indicators)\n",
    "        print(f\"   • Most common indicators:\")\n",
    "        for indicator, count in indicator_counter.most_common(3):\n",
    "            percentage = (count / len(successful_ast)) * 100\n",
    "            print(f\"      • {indicator}: {count}/{len(successful_ast)} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # CWE-specific analysis\n",
    "        print(f\"\\n AST DETECTION BY CWE TYPE:\")\n",
    "        for cwe in successful_ast['cwe'].unique():\n",
    "            cwe_data = successful_ast[successful_ast['cwe'] == cwe]\n",
    "            cwe_indicators = cwe_data[cwe_data['indicator_count'] > 0]\n",
    "            cwe_detection_rate = len(cwe_indicators) / len(cwe_data) * 100\n",
    "            \n",
    "            print(f\"   • {cwe}: {len(cwe_indicators)}/{len(cwe_data)} ({cwe_detection_rate:.1f}%)\")\n",
    "    \n",
    "    # Save results\n",
    "    ast_df.to_csv(results_dir / 'ast_only_analysis.csv', index=False)\n",
    "    print(f\"\\n💾 AST-only analysis saved to: {results_dir / 'ast_only_analysis.csv'}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No AST analysis results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c7eca",
   "metadata": {},
   "source": [
    "## 4.3 Control Flow Complexity Analysis\n",
    "\n",
    "Analyze how many functions have complex control flow that might require CFG analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "418c6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Testing control flow complexity...\n",
      "🔄 ANALYZING CONTROL FLOW COMPLEXITY\n",
      "============================================================\n",
      "📋 Processing sample 1/2317...\n",
      "📋 Processing sample 21/2317...\n",
      "📋 Processing sample 41/2317...\n",
      "📋 Processing sample 61/2317...\n",
      "📋 Processing sample 81/2317...\n",
      "📋 Processing sample 101/2317...\n",
      "📋 Processing sample 121/2317...\n",
      "📋 Processing sample 141/2317...\n",
      "📋 Processing sample 161/2317...\n",
      "📋 Processing sample 181/2317...\n",
      "📋 Processing sample 201/2317...\n",
      "📋 Processing sample 221/2317...\n",
      "📋 Processing sample 241/2317...\n",
      "📋 Processing sample 261/2317...\n",
      "📋 Processing sample 281/2317...\n",
      "📋 Processing sample 301/2317...\n",
      "📋 Processing sample 321/2317...\n",
      "📋 Processing sample 341/2317...\n",
      "📋 Processing sample 361/2317...\n",
      "📋 Processing sample 381/2317...\n",
      "📋 Processing sample 401/2317...\n",
      "📋 Processing sample 421/2317...\n",
      "📋 Processing sample 441/2317...\n",
      "📋 Processing sample 461/2317...\n",
      "📋 Processing sample 481/2317...\n",
      "📋 Processing sample 501/2317...\n",
      "📋 Processing sample 521/2317...\n",
      "📋 Processing sample 541/2317...\n",
      "📋 Processing sample 561/2317...\n",
      "📋 Processing sample 581/2317...\n",
      "📋 Processing sample 601/2317...\n",
      "📋 Processing sample 621/2317...\n",
      "📋 Processing sample 641/2317...\n",
      "📋 Processing sample 661/2317...\n",
      "📋 Processing sample 681/2317...\n",
      "📋 Processing sample 701/2317...\n",
      "📋 Processing sample 721/2317...\n",
      "📋 Processing sample 741/2317...\n",
      "📋 Processing sample 761/2317...\n",
      "📋 Processing sample 781/2317...\n",
      "📋 Processing sample 801/2317...\n",
      "📋 Processing sample 821/2317...\n",
      "📋 Processing sample 841/2317...\n",
      "📋 Processing sample 861/2317...\n",
      "📋 Processing sample 881/2317...\n",
      "📋 Processing sample 901/2317...\n",
      "📋 Processing sample 921/2317...\n",
      "📋 Processing sample 941/2317...\n",
      "📋 Processing sample 961/2317...\n",
      "📋 Processing sample 981/2317...\n",
      "📋 Processing sample 1001/2317...\n",
      "📋 Processing sample 1021/2317...\n",
      "📋 Processing sample 1041/2317...\n",
      "📋 Processing sample 1061/2317...\n",
      "📋 Processing sample 1081/2317...\n",
      "📋 Processing sample 1101/2317...\n",
      "📋 Processing sample 1121/2317...\n",
      "📋 Processing sample 1141/2317...\n",
      "📋 Processing sample 1161/2317...\n",
      "📋 Processing sample 1181/2317...\n",
      "📋 Processing sample 1201/2317...\n",
      "📋 Processing sample 1221/2317...\n",
      "📋 Processing sample 1241/2317...\n",
      "📋 Processing sample 1261/2317...\n",
      "📋 Processing sample 1281/2317...\n",
      "📋 Processing sample 1301/2317...\n",
      "📋 Processing sample 1321/2317...\n",
      "📋 Processing sample 1341/2317...\n",
      "📋 Processing sample 1361/2317...\n",
      "📋 Processing sample 1381/2317...\n",
      "📋 Processing sample 1401/2317...\n",
      "📋 Processing sample 1421/2317...\n",
      "📋 Processing sample 1441/2317...\n",
      "📋 Processing sample 1461/2317...\n",
      "📋 Processing sample 1481/2317...\n",
      "📋 Processing sample 1501/2317...\n",
      "📋 Processing sample 1521/2317...\n",
      "📋 Processing sample 1541/2317...\n",
      "📋 Processing sample 1561/2317...\n",
      "📋 Processing sample 1581/2317...\n",
      "📋 Processing sample 1601/2317...\n",
      "📋 Processing sample 1621/2317...\n",
      "📋 Processing sample 1641/2317...\n",
      "📋 Processing sample 1661/2317...\n",
      "📋 Processing sample 1681/2317...\n",
      "📋 Processing sample 1701/2317...\n",
      "📋 Processing sample 1721/2317...\n",
      "📋 Processing sample 1741/2317...\n",
      "📋 Processing sample 1761/2317...\n",
      "📋 Processing sample 1781/2317...\n",
      "📋 Processing sample 1801/2317...\n",
      "📋 Processing sample 1821/2317...\n",
      "📋 Processing sample 1841/2317...\n",
      "📋 Processing sample 1861/2317...\n",
      "📋 Processing sample 1881/2317...\n",
      "📋 Processing sample 1901/2317...\n",
      "📋 Processing sample 1921/2317...\n",
      "📋 Processing sample 1941/2317...\n",
      "📋 Processing sample 1961/2317...\n",
      "📋 Processing sample 1981/2317...\n",
      "📋 Processing sample 2001/2317...\n",
      "📋 Processing sample 2021/2317...\n",
      "📋 Processing sample 2041/2317...\n",
      "📋 Processing sample 2061/2317...\n",
      "📋 Processing sample 2081/2317...\n",
      "📋 Processing sample 2101/2317...\n",
      "📋 Processing sample 2121/2317...\n",
      "📋 Processing sample 2141/2317...\n",
      "📋 Processing sample 2161/2317...\n",
      "📋 Processing sample 2181/2317...\n",
      "📋 Processing sample 2201/2317...\n",
      "📋 Processing sample 2221/2317...\n",
      "📋 Processing sample 2241/2317...\n",
      "📋 Processing sample 2261/2317...\n",
      "📋 Processing sample 2281/2317...\n",
      "📋 Processing sample 2301/2317...\n",
      "\n",
      "📊 CONTROL FLOW COMPLEXITY RESULTS:\n",
      "   • Total samples: 2317\n",
      "   • CFG construction success: 2317/2317 (100.0%)\n",
      "\n",
      " COMPLEXITY STATISTICS:\n",
      "   • Mean complexity: 1.00\n",
      "   • Median complexity: 1.00\n",
      "   • 95th percentile: 1.00\n",
      "   • Max complexity: 1.00\n",
      "   • Min complexity: 0.00\n",
      "\n",
      " COMPLEXITY DISTRIBUTION:\n",
      "   • Simple (≤3): 2317/2317 (100.0%)\n",
      "   • Moderate (4-10): 0/2317 (0.0%)\n",
      "   • Complex (>10): 0/2317 (0.0%)\n",
      "\n",
      "🔍 COMPLEXITY BY CWE TYPE:\n",
      "   • CWE-119: avg 1.0, 0/173 complex (0.0%)\n",
      "   • CWE-125: avg 1.0, 0/140 complex (0.0%)\n",
      "   • CWE-200: avg 1.0, 0/153 complex (0.0%)\n",
      "   • CWE-20: avg 1.0, 0/182 complex (0.0%)\n",
      "   • CWE-264: avg 1.0, 0/120 complex (0.0%)\n",
      "   • CWE-362: avg 1.0, 0/320 complex (0.0%)\n",
      "   • CWE-401: avg 1.0, 0/101 complex (0.0%)\n",
      "   • CWE-416: avg 1.0, 0/660 complex (0.0%)\n",
      "   • CWE-476: avg 1.0, 0/281 complex (0.0%)\n",
      "   • CWE-787: avg 1.0, 0/187 complex (0.0%)\n",
      "\n",
      "🎯 CFG NECESSITY ANALYSIS:\n",
      "   • Samples requiring CFG analysis: 0/2317 (0.0%)\n",
      "   • RECOMMENDATION: AST-only may be sufficient for most cases\n",
      "\n",
      "💾 Control flow analysis saved to: ../results/control_flow_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "def analyze_control_flow_complexity(samples):\n",
    "    \"\"\"Analyze control flow complexity to determine CFG necessity\"\"\"\n",
    "    print(\"🔄 ANALYZING CONTROL FLOW COMPLEXITY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cf_analysis = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"📋 Processing sample {i+1}/{len(samples)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Build CFG - CORRECTION: use build_simple_cfg with timeout_seconds\n",
    "            cfg_result = build_simple_cfg(sample['code'], timeout_seconds=10)\n",
    "            \n",
    "            if cfg_result.get('success'):\n",
    "                # Extract complexity from global stats\n",
    "                global_stats = cfg_result.get('global_stats', {})\n",
    "                total_nodes = global_stats.get('total_nodes', 0)\n",
    "                total_edges = global_stats.get('total_edges', 0)\n",
    "                \n",
    "                # Calculate cyclomatic complexity (edges - nodes + 2)\n",
    "                cyclomatic_complexity = total_edges - total_nodes + 2 if total_nodes > 0 else 1\n",
    "                \n",
    "                cf_analysis.append({\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'cfg_nodes': total_nodes,\n",
    "                    'cfg_edges': total_edges,\n",
    "                    'cyclomatic_complexity': cyclomatic_complexity,\n",
    "                    'cfg_success': True\n",
    "                })\n",
    "            else:\n",
    "                cf_analysis.append({\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'cfg_nodes': 0,\n",
    "                    'cfg_edges': 0,\n",
    "                    'cyclomatic_complexity': 0,\n",
    "                    'cfg_success': False\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error analyzing CFG for sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return cf_analysis\n",
    "\n",
    "# Analyze control flow complexity\n",
    "print(\"🔄 Testing control flow complexity...\")\n",
    "cf_analysis = analyze_control_flow_complexity(analysis_samples)\n",
    "\n",
    "# Analyze results\n",
    "if cf_analysis:\n",
    "    cf_df = pd.DataFrame(cf_analysis)\n",
    "    successful_cfg = cf_df[cf_df['cfg_success']]\n",
    "    \n",
    "    print(f\"\\n📊 CONTROL FLOW COMPLEXITY RESULTS:\")\n",
    "    print(f\"   • Total samples: {len(cf_df)}\")\n",
    "    print(f\"   • CFG construction success: {len(successful_cfg)}/{len(cf_df)} ({len(successful_cfg)/len(cf_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_cfg) > 0:\n",
    "        # Complexity distribution - CORRECTION: Handle percentiles safely\n",
    "        complexity_values = successful_cfg['cyclomatic_complexity'].values\n",
    "        \n",
    "        print(f\"\\n COMPLEXITY STATISTICS:\")\n",
    "        print(f\"   • Mean complexity: {complexity_values.mean():.2f}\")\n",
    "        print(f\"   • Median complexity: {np.median(complexity_values):.2f}\")\n",
    "        \n",
    "        # Calculate 95th percentile safely\n",
    "        if len(complexity_values) > 1:\n",
    "            percentile_95 = np.percentile(complexity_values, 95)\n",
    "            print(f\"   • 95th percentile: {percentile_95:.2f}\")\n",
    "        else:\n",
    "            print(f\"   • 95th percentile: {complexity_values[0]:.2f} (single value)\")\n",
    "            \n",
    "        print(f\"   • Max complexity: {complexity_values.max():.2f}\")\n",
    "        print(f\"   • Min complexity: {complexity_values.min():.2f}\")\n",
    "        \n",
    "        # Complexity categories\n",
    "        simple_cfg = successful_cfg[successful_cfg['cyclomatic_complexity'] <= 3]\n",
    "        moderate_cfg = successful_cfg[(successful_cfg['cyclomatic_complexity'] > 3) & \n",
    "                                    (successful_cfg['cyclomatic_complexity'] <= 10)]\n",
    "        complex_cfg = successful_cfg[successful_cfg['cyclomatic_complexity'] > 10]\n",
    "        \n",
    "        print(f\"\\n COMPLEXITY DISTRIBUTION:\")\n",
    "        print(f\"   • Simple (≤3): {len(simple_cfg)}/{len(successful_cfg)} ({len(simple_cfg)/len(successful_cfg)*100:.1f}%)\")\n",
    "        print(f\"   • Moderate (4-10): {len(moderate_cfg)}/{len(successful_cfg)} ({len(moderate_cfg)/len(successful_cfg)*100:.1f}%)\")\n",
    "        print(f\"   • Complex (>10): {len(complex_cfg)}/{len(successful_cfg)} ({len(complex_cfg)/len(successful_cfg)*100:.1f}%)\")\n",
    "        \n",
    "        # CWE-specific complexity\n",
    "        print(f\"\\n🔍 COMPLEXITY BY CWE TYPE:\")\n",
    "        for cwe in successful_cfg['cwe'].unique():\n",
    "            cwe_data = successful_cfg[successful_cfg['cwe'] == cwe]\n",
    "            avg_complexity = cwe_data['cyclomatic_complexity'].mean()\n",
    "            complex_count = len(cwe_data[cwe_data['cyclomatic_complexity'] > 10])\n",
    "            complex_rate = complex_count / len(cwe_data) * 100\n",
    "            \n",
    "            print(f\"   • {cwe}: avg {avg_complexity:.1f}, {complex_count}/{len(cwe_data)} complex ({complex_rate:.1f}%)\")\n",
    "        \n",
    "        # Determine CFG necessity\n",
    "        complex_threshold = 10  # Arbitrary threshold for \"complex\" control flow\n",
    "        complex_samples = successful_cfg[successful_cfg['cyclomatic_complexity'] > complex_threshold]\n",
    "        cfg_necessity_rate = len(complex_samples) / len(successful_cfg) * 100\n",
    "        \n",
    "        print(f\"\\n🎯 CFG NECESSITY ANALYSIS:\")\n",
    "        print(f\"   • Samples requiring CFG analysis: {len(complex_samples)}/{len(successful_cfg)} ({cfg_necessity_rate:.1f}%)\")\n",
    "        \n",
    "        if cfg_necessity_rate > 50:\n",
    "            print(\"   • RECOMMENDATION: CFG analysis is important for this dataset\")\n",
    "        elif cfg_necessity_rate > 20:\n",
    "            print(\"   • RECOMMENDATION: CFG analysis provides moderate value\")\n",
    "        else:\n",
    "            print(\"   • RECOMMENDATION: AST-only may be sufficient for most cases\")\n",
    "    \n",
    "    # Save results\n",
    "    cf_df.to_csv(results_dir / 'control_flow_analysis.csv', index=False)\n",
    "    print(f\"\\n💾 Control flow analysis saved to: {results_dir / 'control_flow_analysis.csv'}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No control flow analysis results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e07584",
   "metadata": {},
   "source": [
    "## 4.4 Data Dependency Requirements\n",
    "\n",
    "Analyze if vulnerabilities require data flow analysis (PDG) or if simpler approaches suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc8e0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Testing data dependency requirements...\n",
      " ANALYZING DATA DEPENDENCY REQUIREMENTS\n",
      "============================================================\n",
      "📋 Processing sample 1/2317...\n",
      "📋 Processing sample 21/2317...\n",
      "📋 Processing sample 41/2317...\n",
      "📋 Processing sample 61/2317...\n",
      "📋 Processing sample 81/2317...\n",
      "📋 Processing sample 101/2317...\n",
      "📋 Processing sample 121/2317...\n",
      "📋 Processing sample 141/2317...\n",
      "📋 Processing sample 161/2317...\n",
      "📋 Processing sample 181/2317...\n",
      "📋 Processing sample 201/2317...\n",
      "📋 Processing sample 221/2317...\n",
      "📋 Processing sample 241/2317...\n",
      "📋 Processing sample 261/2317...\n",
      "📋 Processing sample 281/2317...\n",
      "📋 Processing sample 301/2317...\n",
      "📋 Processing sample 321/2317...\n",
      "📋 Processing sample 341/2317...\n",
      "📋 Processing sample 361/2317...\n",
      "📋 Processing sample 381/2317...\n",
      "📋 Processing sample 401/2317...\n",
      "📋 Processing sample 421/2317...\n",
      "📋 Processing sample 441/2317...\n",
      "📋 Processing sample 461/2317...\n",
      "📋 Processing sample 481/2317...\n",
      "📋 Processing sample 501/2317...\n",
      "📋 Processing sample 521/2317...\n",
      "📋 Processing sample 541/2317...\n",
      "📋 Processing sample 561/2317...\n",
      "📋 Processing sample 581/2317...\n",
      "📋 Processing sample 601/2317...\n",
      "📋 Processing sample 621/2317...\n",
      "📋 Processing sample 641/2317...\n",
      "📋 Processing sample 661/2317...\n",
      "📋 Processing sample 681/2317...\n",
      "📋 Processing sample 701/2317...\n",
      "📋 Processing sample 721/2317...\n",
      "📋 Processing sample 741/2317...\n",
      "📋 Processing sample 761/2317...\n",
      "📋 Processing sample 781/2317...\n",
      "📋 Processing sample 801/2317...\n",
      "📋 Processing sample 821/2317...\n",
      "📋 Processing sample 841/2317...\n",
      "📋 Processing sample 861/2317...\n",
      "📋 Processing sample 881/2317...\n",
      "📋 Processing sample 901/2317...\n",
      "📋 Processing sample 921/2317...\n",
      "📋 Processing sample 941/2317...\n",
      "📋 Processing sample 961/2317...\n",
      "📋 Processing sample 981/2317...\n",
      "📋 Processing sample 1001/2317...\n",
      "📋 Processing sample 1021/2317...\n",
      "📋 Processing sample 1041/2317...\n",
      "📋 Processing sample 1061/2317...\n",
      "📋 Processing sample 1081/2317...\n",
      "📋 Processing sample 1101/2317...\n",
      "📋 Processing sample 1121/2317...\n",
      "📋 Processing sample 1141/2317...\n",
      "📋 Processing sample 1161/2317...\n",
      "📋 Processing sample 1181/2317...\n",
      "📋 Processing sample 1201/2317...\n",
      "📋 Processing sample 1221/2317...\n",
      "📋 Processing sample 1241/2317...\n",
      "📋 Processing sample 1261/2317...\n",
      "📋 Processing sample 1281/2317...\n",
      "📋 Processing sample 1301/2317...\n",
      "📋 Processing sample 1321/2317...\n",
      "📋 Processing sample 1341/2317...\n",
      "📋 Processing sample 1361/2317...\n",
      "📋 Processing sample 1381/2317...\n",
      "📋 Processing sample 1401/2317...\n",
      "📋 Processing sample 1421/2317...\n",
      "📋 Processing sample 1441/2317...\n",
      "📋 Processing sample 1461/2317...\n",
      "📋 Processing sample 1481/2317...\n",
      "📋 Processing sample 1501/2317...\n",
      "📋 Processing sample 1521/2317...\n",
      "📋 Processing sample 1541/2317...\n",
      "📋 Processing sample 1561/2317...\n",
      "📋 Processing sample 1581/2317...\n",
      "📋 Processing sample 1601/2317...\n",
      "📋 Processing sample 1621/2317...\n",
      "📋 Processing sample 1641/2317...\n",
      "📋 Processing sample 1661/2317...\n",
      "📋 Processing sample 1681/2317...\n",
      "📋 Processing sample 1701/2317...\n",
      "📋 Processing sample 1721/2317...\n",
      "📋 Processing sample 1741/2317...\n",
      "📋 Processing sample 1761/2317...\n",
      "📋 Processing sample 1781/2317...\n",
      "📋 Processing sample 1801/2317...\n",
      "📋 Processing sample 1821/2317...\n",
      "📋 Processing sample 1841/2317...\n",
      "📋 Processing sample 1861/2317...\n",
      "📋 Processing sample 1881/2317...\n",
      "📋 Processing sample 1901/2317...\n",
      "📋 Processing sample 1921/2317...\n",
      "📋 Processing sample 1941/2317...\n",
      "📋 Processing sample 1961/2317...\n",
      "📋 Processing sample 1981/2317...\n",
      "📋 Processing sample 2001/2317...\n",
      "📋 Processing sample 2021/2317...\n",
      "📋 Processing sample 2041/2317...\n",
      "📋 Processing sample 2061/2317...\n",
      "📋 Processing sample 2081/2317...\n",
      "📋 Processing sample 2101/2317...\n",
      "📋 Processing sample 2121/2317...\n",
      "📋 Processing sample 2141/2317...\n",
      "📋 Processing sample 2161/2317...\n",
      "📋 Processing sample 2181/2317...\n",
      "📋 Processing sample 2201/2317...\n",
      "📋 Processing sample 2221/2317...\n",
      "📋 Processing sample 2241/2317...\n",
      "📋 Processing sample 2261/2317...\n",
      "📋 Processing sample 2281/2317...\n",
      "📋 Processing sample 2301/2317...\n",
      "\n",
      "📊 DATA DEPENDENCY ANALYSIS RESULTS:\n",
      "   • Total samples: 2317\n",
      "   • PDG construction success: 2317/2317 (100.0%)\n",
      "\n",
      " DEPENDENCY STATISTICS:\n",
      "   • Mean dependencies: 44.60\n",
      "   • Median dependencies: 15.00\n",
      "   • 95th percentile: 166.00\n",
      "   • Max dependencies: 2311.00\n",
      "   • Min dependencies: 0.00\n",
      "\n",
      "📊 DATA FLOW REQUIREMENTS:\n",
      "   • Samples with data dependencies: 1656/2317 (71.5%)\n",
      "\n",
      "🔍 DATA FLOW BY CWE TYPE:\n",
      "   • CWE-119: 131/173 with flow (75.7%), avg 53.6 deps\n",
      "   • CWE-125: 110/140 with flow (78.6%), avg 60.7 deps\n",
      "   • CWE-200: 107/153 with flow (69.9%), avg 38.3 deps\n",
      "   • CWE-20: 141/182 with flow (77.5%), avg 52.8 deps\n",
      "   • CWE-264: 81/120 with flow (67.5%), avg 31.1 deps\n",
      "   • CWE-362: 216/320 with flow (67.5%), avg 41.9 deps\n",
      "   • CWE-401: 69/101 with flow (68.3%), avg 38.4 deps\n",
      "   • CWE-416: 451/660 with flow (68.3%), avg 37.2 deps\n",
      "   • CWE-476: 200/281 with flow (71.2%), avg 44.8 deps\n",
      "   • CWE-787: 150/187 with flow (80.2%), avg 64.0 deps\n",
      "\n",
      "🎯 PDG NECESSITY ANALYSIS:\n",
      "   • Samples requiring PDG analysis: 1535/2317 (66.2%)\n",
      "   • RECOMMENDATION: PDG analysis is important for this dataset\n",
      "\n",
      "💾 Data dependency analysis saved to: ../results/data_dependency_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_dependency_requirements(samples):\n",
    "    \"\"\"Analyze data dependency requirements to determine PDG necessity\"\"\"\n",
    "    print(\" ANALYZING DATA DEPENDENCY REQUIREMENTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    pdg_analysis = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"📋 Processing sample {i+1}/{len(samples)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Build PDG - CORRECTION: use build_simple_pdg with timeout_seconds\n",
    "            pdg_result = build_simple_pdg(sample['code'], timeout_seconds=10)\n",
    "            \n",
    "            if pdg_result.get('success'):\n",
    "                # Extract dependency count from global stats\n",
    "                global_stats = pdg_result.get('global_stats', {})\n",
    "                dependency_count = global_stats.get('total_dependencies', 0)\n",
    "                \n",
    "                pdg_analysis.append({\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'dependency_count': dependency_count,\n",
    "                    'pdg_success': True,\n",
    "                    'has_data_flow': dependency_count > 0\n",
    "                })\n",
    "            else:\n",
    "                pdg_analysis.append({\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'dependency_count': 0,\n",
    "                    'pdg_success': False,\n",
    "                    'has_data_flow': False\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error analyzing PDG for sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pdg_analysis\n",
    "\n",
    "# Analyze data dependencies\n",
    "print(\"🔗 Testing data dependency requirements...\")\n",
    "pdg_analysis = analyze_data_dependency_requirements(analysis_samples)\n",
    "\n",
    "# Analyze results\n",
    "if pdg_analysis:\n",
    "    pdg_df = pd.DataFrame(pdg_analysis)\n",
    "    successful_pdg = pdg_df[pdg_df['pdg_success']]\n",
    "    \n",
    "    print(f\"\\n📊 DATA DEPENDENCY ANALYSIS RESULTS:\")\n",
    "    print(f\"   • Total samples: {len(pdg_df)}\")\n",
    "    print(f\"   • PDG construction success: {len(successful_pdg)}/{len(pdg_df)} ({len(successful_pdg)/len(pdg_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_pdg) > 0:\n",
    "        # Dependency statistics - CORRECTION: Handle percentiles safely\n",
    "        dependency_values = successful_pdg['dependency_count'].values\n",
    "        \n",
    "        print(f\"\\n DEPENDENCY STATISTICS:\")\n",
    "        print(f\"   • Mean dependencies: {dependency_values.mean():.2f}\")\n",
    "        print(f\"   • Median dependencies: {np.median(dependency_values):.2f}\")\n",
    "        \n",
    "        # Calculate 95th percentile safely\n",
    "        if len(dependency_values) > 1:\n",
    "            percentile_95 = np.percentile(dependency_values, 95)\n",
    "            print(f\"   • 95th percentile: {percentile_95:.2f}\")\n",
    "        else:\n",
    "            print(f\"   • 95th percentile: {dependency_values[0]:.2f} (single value)\")\n",
    "            \n",
    "        print(f\"   • Max dependencies: {dependency_values.max():.2f}\")\n",
    "        print(f\"   • Min dependencies: {dependency_values.min():.2f}\")\n",
    "        \n",
    "        # Samples with data flow\n",
    "        samples_with_flow = successful_pdg[successful_pdg['has_data_flow']]\n",
    "        data_flow_rate = len(samples_with_flow) / len(successful_pdg) * 100\n",
    "        \n",
    "        print(f\"\\n📊 DATA FLOW REQUIREMENTS:\")\n",
    "        print(f\"   • Samples with data dependencies: {len(samples_with_flow)}/{len(successful_pdg)} ({data_flow_rate:.1f}%)\")\n",
    "        \n",
    "        # CWE-specific data flow requirements\n",
    "        print(f\"\\n🔍 DATA FLOW BY CWE TYPE:\")\n",
    "        for cwe in successful_pdg['cwe'].unique():\n",
    "            cwe_data = successful_pdg[successful_pdg['cwe'] == cwe]\n",
    "            cwe_flow = cwe_data[cwe_data['has_data_flow']]\n",
    "            cwe_flow_rate = len(cwe_flow) / len(cwe_data) * 100\n",
    "            avg_deps = cwe_data['dependency_count'].mean()\n",
    "            \n",
    "            print(f\"   • {cwe}: {len(cwe_flow)}/{len(cwe_data)} with flow ({cwe_flow_rate:.1f}%), avg {avg_deps:.1f} deps\")\n",
    "        \n",
    "        # Determine PDG necessity\n",
    "        dependency_threshold = 5  # Arbitrary threshold for \"significant\" dependencies\n",
    "        significant_deps = successful_pdg[successful_pdg['dependency_count'] >= dependency_threshold]\n",
    "        pdg_necessity_rate = len(significant_deps) / len(successful_pdg) * 100\n",
    "        \n",
    "        print(f\"\\n🎯 PDG NECESSITY ANALYSIS:\")\n",
    "        print(f\"   • Samples requiring PDG analysis: {len(significant_deps)}/{len(successful_pdg)} ({pdg_necessity_rate:.1f}%)\")\n",
    "        \n",
    "        if pdg_necessity_rate > 50:\n",
    "            print(\"   • RECOMMENDATION: PDG analysis is important for this dataset\")\n",
    "        elif pdg_necessity_rate > 20:\n",
    "            print(\"   • RECOMMENDATION: PDG analysis provides moderate value\")\n",
    "        else:\n",
    "            print(\"   • RECOMMENDATION: AST-only may be sufficient for most cases\")\n",
    "    \n",
    "    # Save results\n",
    "    pdg_df.to_csv(results_dir / 'data_dependency_analysis.csv', index=False)\n",
    "    print(f\"\\n💾 Data dependency analysis saved to: {results_dir / 'data_dependency_analysis.csv'}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No data dependency analysis results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5e811",
   "metadata": {},
   "source": [
    "## 4.5 Architecture Decision Analysis\n",
    "\n",
    "Compare the effectiveness and cost of different architectural approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12372da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏗️ COMPARING ARCHITECTURAL APPROACHES\n",
      "============================================================\n",
      "📊 ARCHITECTURE COMPARISON RESULTS:\n",
      "   • Total samples analyzed: 2413\n",
      "\n",
      "✅ SUCCESS RATES:\n",
      "   • AST extraction: 100.0%\n",
      "   • CFG construction: 100.0%\n",
      "   • PDG construction: 100.0%\n",
      "   • AST vulnerability detection: 96.9%\n",
      "   • Complex control flow requiring CFG: 0.0%\n",
      "   • Significant data dependencies requiring PDG: 64.4%\n",
      "\n",
      "🎯 ARCHITECTURE RECOMMENDATIONS:\n",
      "   • RECOMMENDED ARCHITECTURE: AST + PDG\n",
      "   • REASONING: Low control flow complexity, high data dependency requirements\n",
      "\n",
      "💰 COST-BENEFIT ANALYSIS:\n",
      "   • AST-only processing time: 1.0ms\n",
      "   • AST+CFG processing time: 6.0ms\n",
      "   • AST+PDG processing time: 11.0ms\n",
      "   • Full AST+CFG+PDG time: 16.0ms\n",
      "   • Recommended effectiveness: 100.0%\n",
      "   • Processing efficiency: 9.1% per ms\n",
      "\n",
      "💾 Architecture decision saved to: ../results/architecture_decision.json\n"
     ]
    }
   ],
   "source": [
    "def compare_architectural_approaches(ast_analysis, cf_analysis, pdg_analysis):\n",
    "    \"\"\"Compare different architectural approaches\"\"\"\n",
    "    print(\"🏗️ COMPARING ARCHITECTURAL APPROACHES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    ast_df = pd.DataFrame(ast_analysis) if ast_analysis else pd.DataFrame()\n",
    "    cf_df = pd.DataFrame(cf_analysis) if cf_analysis else pd.DataFrame()\n",
    "    pdg_df = pd.DataFrame(pdg_analysis) if pdg_analysis else pd.DataFrame()\n",
    "    \n",
    "    # Merge results by CVE ID\n",
    "    if not ast_df.empty and not cf_df.empty and not pdg_df.empty:\n",
    "        # Merge all analyses\n",
    "        merged_df = ast_df.merge(cf_df, on=['cwe', 'cve_id', 'lines', 'chars'], suffixes=('_ast', '_cfg'))\n",
    "        merged_df = merged_df.merge(pdg_df, on=['cwe', 'cve_id', 'lines', 'chars'], suffixes=('', '_pdg'))\n",
    "        \n",
    "        print(f\"📊 ARCHITECTURE COMPARISON RESULTS:\")\n",
    "        print(f\"   • Total samples analyzed: {len(merged_df)}\")\n",
    "        \n",
    "        # Success rates\n",
    "        ast_success = merged_df['ast_success'].sum() / len(merged_df) * 100\n",
    "        cfg_success = merged_df['cfg_success'].sum() / len(merged_df) * 100\n",
    "        pdg_success = merged_df['pdg_success'].sum() / len(merged_df) * 100\n",
    "        \n",
    "        print(f\"\\n✅ SUCCESS RATES:\")\n",
    "        print(f\"   • AST extraction: {ast_success:.1f}%\")\n",
    "        print(f\"   • CFG construction: {cfg_success:.1f}%\")\n",
    "        print(f\"   • PDG construction: {pdg_success:.1f}%\")\n",
    "        \n",
    "        # Detection capabilities\n",
    "        if 'indicator_count' in merged_df.columns:\n",
    "            ast_detection = (merged_df['indicator_count'] > 0).sum() / len(merged_df) * 100\n",
    "            print(f\"   • AST vulnerability detection: {ast_detection:.1f}%\")\n",
    "        \n",
    "        # Complexity analysis\n",
    "        if 'cyclomatic_complexity' in merged_df.columns:\n",
    "            complex_cfg = (merged_df['cyclomatic_complexity'] > 10).sum() / len(merged_df) * 100\n",
    "            print(f\"   • Complex control flow requiring CFG: {complex_cfg:.1f}%\")\n",
    "        \n",
    "        # Data flow analysis\n",
    "        if 'dependency_count' in merged_df.columns:\n",
    "            significant_pdg = (merged_df['dependency_count'] >= 5).sum() / len(merged_df) * 100\n",
    "            print(f\"   • Significant data dependencies requiring PDG: {significant_pdg:.1f}%\")\n",
    "        \n",
    "        # Architecture recommendations\n",
    "        print(f\"\\n🎯 ARCHITECTURE RECOMMENDATIONS:\")\n",
    "        \n",
    "        # Determine optimal architecture\n",
    "        if ast_success > 95 and ast_detection > 80:\n",
    "            if complex_cfg < 30 and significant_pdg < 30:\n",
    "                recommendation = \"AST-ONLY\"\n",
    "                reasoning = \"High AST success rate and detection capability, low complexity requirements\"\n",
    "            elif complex_cfg > 50 and significant_pdg < 30:\n",
    "                recommendation = \"AST + CFG\"\n",
    "                reasoning = \"High control flow complexity, low data dependency requirements\"\n",
    "            elif complex_cfg < 30 and significant_pdg > 50:\n",
    "                recommendation = \"AST + PDG\"\n",
    "                reasoning = \"Low control flow complexity, high data dependency requirements\"\n",
    "            else:\n",
    "                recommendation = \"AST + CFG + PDG\"\n",
    "                reasoning = \"High complexity in both control flow and data dependencies\"\n",
    "        else:\n",
    "            recommendation = \"AST + CFG + PDG\"\n",
    "            reasoning = \"AST alone insufficient, full analysis recommended\"\n",
    "        \n",
    "        print(f\"   • RECOMMENDED ARCHITECTURE: {recommendation}\")\n",
    "        print(f\"   • REASONING: {reasoning}\")\n",
    "        \n",
    "        # Cost-benefit analysis\n",
    "        print(f\"\\n💰 COST-BENEFIT ANALYSIS:\")\n",
    "        \n",
    "        # Processing time estimates (from Phase 2)\n",
    "        ast_time = 0.001  # 1ms\n",
    "        cfg_time = 0.005  # 5ms\n",
    "        pdg_time = 0.010  # 10ms\n",
    "        \n",
    "        ast_only_cost = ast_time\n",
    "        ast_cfg_cost = ast_time + cfg_time\n",
    "        ast_pdg_cost = ast_time + pdg_time\n",
    "        full_cost = ast_time + cfg_time + pdg_time\n",
    "        \n",
    "        print(f\"   • AST-only processing time: {ast_only_cost*1000:.1f}ms\")\n",
    "        print(f\"   • AST+CFG processing time: {ast_cfg_cost*1000:.1f}ms\")\n",
    "        print(f\"   • AST+PDG processing time: {ast_pdg_cost*1000:.1f}ms\")\n",
    "        print(f\"   • Full AST+CFG+PDG time: {full_cost*1000:.1f}ms\")\n",
    "        \n",
    "        # Effectiveness vs cost\n",
    "        if recommendation == \"AST-ONLY\":\n",
    "            effectiveness = ast_detection\n",
    "            cost = ast_only_cost\n",
    "        elif recommendation == \"AST + CFG\":\n",
    "            effectiveness = min(100, ast_detection + complex_cfg * 0.5)\n",
    "            cost = ast_cfg_cost\n",
    "        elif recommendation == \"AST + PDG\":\n",
    "            effectiveness = min(100, ast_detection + significant_pdg * 0.5)\n",
    "            cost = ast_pdg_cost\n",
    "        else:\n",
    "            effectiveness = 95  # Estimated full effectiveness\n",
    "            cost = full_cost\n",
    "        \n",
    "        efficiency = effectiveness / (cost * 1000)  # Effectiveness per millisecond\n",
    "        print(f\"   • Recommended effectiveness: {effectiveness:.1f}%\")\n",
    "        print(f\"   • Processing efficiency: {efficiency:.1f}% per ms\")\n",
    "        \n",
    "        # Save architecture analysis\n",
    "        architecture_summary = {\n",
    "            \"recommendation\": recommendation,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"success_rates\": {\n",
    "                \"ast\": ast_success,\n",
    "                \"cfg\": cfg_success,\n",
    "                \"pdg\": pdg_success\n",
    "            },\n",
    "            \"complexity_requirements\": {\n",
    "                \"complex_cfg_rate\": complex_cfg,\n",
    "                \"significant_pdg_rate\": significant_pdg\n",
    "            },\n",
    "            \"cost_analysis\": {\n",
    "                \"ast_only_ms\": ast_only_cost * 1000,\n",
    "                \"ast_cfg_ms\": ast_cfg_cost * 1000,\n",
    "                \"ast_pdg_ms\": ast_pdg_cost * 1000,\n",
    "                \"full_ms\": full_cost * 1000\n",
    "            },\n",
    "            \"effectiveness\": effectiveness,\n",
    "            \"efficiency\": efficiency\n",
    "        }\n",
    "        \n",
    "        with open(results_dir / 'architecture_decision.json', 'w') as f:\n",
    "            json.dump(architecture_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n💾 Architecture decision saved to: {results_dir / 'architecture_decision.json'}\")\n",
    "        \n",
    "        return architecture_summary\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ Insufficient data for architecture comparison\")\n",
    "        return None\n",
    "\n",
    "# Compare architectural approaches\n",
    "if 'ast_analysis' in locals() and 'cf_analysis' in locals() and 'pdg_analysis' in locals():\n",
    "    architecture_decision = compare_architectural_approaches(ast_analysis, cf_analysis, pdg_analysis)\n",
    "else:\n",
    "    print(\"❌ Missing analysis data for architecture comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b4911",
   "metadata": {},
   "source": [
    "## 4.6 Visualization and Final Recommendations\n",
    "\n",
    "Create visualizations to support the architecture decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3d1c1",
   "metadata": {},
   "source": [
    "##  Phase 4 Completion Summary\n",
    "\n",
    "Final architecture decision and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce1e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🎯 PHASE 4: STRUCTURAL ANALYSIS NECESSITY - COMPLETION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✅ 4.1 VULNERABILITY SAMPLE ANALYSIS COMPLETED:\n",
      "   • 2317 vulnerability samples analyzed\n",
      "   • 10 CWE types covered\n",
      "\n",
      "✅ 4.2 AST-ONLY EFFECTIVENESS ANALYZED:\n",
      "   • AST extraction success: 2317/2317 (100.0%)\n",
      "   • Vulnerability detection rate: 97.1%\n",
      "\n",
      "✅ 4.3 CONTROL FLOW COMPLEXITY ANALYZED:\n",
      "   • CFG construction success: 2317/2317 (100.0%)\n",
      "   • Complex control flow rate: 0.0%\n",
      "\n",
      "✅ 4.4 DATA DEPENDENCY REQUIREMENTS ANALYZED:\n",
      "   • PDG construction success: 2317/2317 (100.0%)\n",
      "   • Significant data dependencies: 66.2%\n",
      "\n",
      "✅ 4.5 ARCHITECTURE DECISION MADE:\n",
      "   • Recommended architecture: AST + PDG\n",
      "   • Expected effectiveness: 100.0%\n",
      "   • Processing efficiency: 9.1% per ms\n",
      "\n",
      "🔬 KEY ARCHITECTURE FINDINGS:\n",
      "   1. AST-only detection is effective (97.1% success rate)\n",
      "   2. Low control flow complexity - CFG may be optional (0.0% complex)\n",
      "   3. High data dependency requirements need PDG analysis (66.2% significant)\n",
      "   4. Optimal architecture determined: AST + PDG\n",
      "\n",
      "📁 GENERATED FILES:\n",
      "   ✅ ast_only_analysis.csv (191.1 KB)\n",
      "   ✅ control_flow_analysis.csv (94.5 KB)\n",
      "   ✅ data_dependency_analysis.csv (97.4 KB)\n",
      "   ✅ architecture_decision.json (0.5 KB)\n",
      "\n",
      " VALIDATION OF ARCHITECTURE DECISIONS:\n",
      "   • ✅ PDG analysis adds value to vulnerability detection\n",
      "   • ✅ Sufficient data for architecture analysis\n",
      "   • ✅ Recommended architecture achieves high effectiveness\n",
      "\n",
      "🚀 NEXT STEPS:\n",
      "   1. Implement recommended architecture in production\n",
      "   2. Validate architecture performance on full dataset\n",
      "   3. Proceed to Phase 5: Configuration Derivation\n",
      "   4. Update project configuration with architecture decision\n",
      "\n",
      " PHASE 4 COMPLETE - ARCHITECTURE DECISION MADE!\n",
      "   AST effectiveness: ANALYZED ✅\n",
      "   CFG necessity: EVALUATED ✅\n",
      "   PDG requirements: ASSESSED ✅\n",
      "   Optimal architecture: DETERMINED ✅\n"
     ]
    }
   ],
   "source": [
    "# Phase 4 completion summary\n",
    "print(\"=\"*80)\n",
    "print(\"🎯 PHASE 4: STRUCTURAL ANALYSIS NECESSITY - COMPLETION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summarize what was accomplished\n",
    "if 'analysis_samples' in locals():\n",
    "    print(f\"\\n✅ 4.1 VULNERABILITY SAMPLE ANALYSIS COMPLETED:\")\n",
    "    print(f\"   • {len(analysis_samples)} vulnerability samples analyzed\")\n",
    "    cwe_counts = Counter(sample['cwe'] for sample in analysis_samples)\n",
    "    print(f\"   • {len(cwe_counts)} CWE types covered\")\n",
    "\n",
    "if 'ast_analysis' in locals() and ast_analysis:\n",
    "    ast_df = pd.DataFrame(ast_analysis)\n",
    "    successful_ast = ast_df[ast_df['ast_success']]\n",
    "    print(f\"\\n✅ 4.2 AST-ONLY EFFECTIVENESS ANALYZED:\")\n",
    "    print(f\"   • AST extraction success: {len(successful_ast)}/{len(ast_df)} ({len(successful_ast)/len(ast_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_ast) > 0:\n",
    "        detection_rate = (successful_ast['indicator_count'] > 0).mean() * 100\n",
    "        print(f\"   • Vulnerability detection rate: {detection_rate:.1f}%\")\n",
    "\n",
    "if 'cf_analysis' in locals() and cf_analysis:\n",
    "    cf_df = pd.DataFrame(cf_analysis)\n",
    "    successful_cfg = cf_df[cf_df['cfg_success']]\n",
    "    print(f\"\\n✅ 4.3 CONTROL FLOW COMPLEXITY ANALYZED:\")\n",
    "    print(f\"   • CFG construction success: {len(successful_cfg)}/{len(cf_df)} ({len(successful_cfg)/len(cf_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_cfg) > 0:\n",
    "        complex_rate = (successful_cfg['cyclomatic_complexity'] > 10).mean() * 100\n",
    "        print(f\"   • Complex control flow rate: {complex_rate:.1f}%\")\n",
    "\n",
    "if 'pdg_analysis' in locals() and pdg_analysis:\n",
    "    pdg_df = pd.DataFrame(pdg_analysis)\n",
    "    successful_pdg = pdg_df[pdg_df['pdg_success']]\n",
    "    print(f\"\\n✅ 4.4 DATA DEPENDENCY REQUIREMENTS ANALYZED:\")\n",
    "    print(f\"   • PDG construction success: {len(successful_pdg)}/{len(pdg_df)} ({len(successful_pdg)/len(pdg_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_pdg) > 0:\n",
    "        significant_rate = (successful_pdg['dependency_count'] >= 5).mean() * 100\n",
    "        print(f\"   • Significant data dependencies: {significant_rate:.1f}%\")\n",
    "\n",
    "if 'architecture_decision' in locals() and architecture_decision:\n",
    "    print(f\"\\n✅ 4.5 ARCHITECTURE DECISION MADE:\")\n",
    "    rec = architecture_decision['recommendation']\n",
    "    effectiveness = architecture_decision['effectiveness']\n",
    "    print(f\"   • Recommended architecture: {rec}\")\n",
    "    print(f\"   • Expected effectiveness: {effectiveness:.1f}%\")\n",
    "    print(f\"   • Processing efficiency: {architecture_decision['efficiency']:.1f}% per ms\")\n",
    "\n",
    "# Key findings\n",
    "print(f\"\\n🔬 KEY ARCHITECTURE FINDINGS:\")\n",
    "\n",
    "findings = []\n",
    "\n",
    "if 'ast_analysis' in locals() and ast_analysis:\n",
    "    ast_df = pd.DataFrame(ast_analysis)\n",
    "    successful_ast = ast_df[ast_df['ast_success']]\n",
    "    if len(successful_ast) > 0:\n",
    "        detection_rate = (successful_ast['indicator_count'] > 0).mean() * 100\n",
    "        if detection_rate > 80:\n",
    "            findings.append(f\"AST-only detection is effective ({detection_rate:.1f}% success rate)\")\n",
    "        else:\n",
    "            findings.append(f\"AST-only detection has limitations ({detection_rate:.1f}% success rate)\")\n",
    "\n",
    "if 'cf_analysis' in locals() and cf_analysis:\n",
    "    cf_df = pd.DataFrame(cf_analysis)\n",
    "    successful_cfg = cf_df[cf_df['cfg_success']]\n",
    "    if len(successful_cfg) > 0:\n",
    "        complex_rate = (successful_cfg['cyclomatic_complexity'] > 10).mean() * 100\n",
    "        if complex_rate > 50:\n",
    "            findings.append(f\"High control flow complexity requires CFG analysis ({complex_rate:.1f}% complex)\")\n",
    "        else:\n",
    "            findings.append(f\"Low control flow complexity - CFG may be optional ({complex_rate:.1f}% complex)\")\n",
    "\n",
    "if 'pdg_analysis' in locals() and pdg_analysis:\n",
    "    pdg_df = pd.DataFrame(pdg_analysis)\n",
    "    successful_pdg = pdg_df[pdg_df['pdg_success']]\n",
    "    if len(successful_pdg) > 0:\n",
    "        significant_rate = (successful_pdg['dependency_count'] >= 5).mean() * 100\n",
    "        if significant_rate > 50:\n",
    "            findings.append(f\"High data dependency requirements need PDG analysis ({significant_rate:.1f}% significant)\")\n",
    "        else:\n",
    "            findings.append(f\"Low data dependency requirements - PDG may be optional ({significant_rate:.1f}% significant)\")\n",
    "\n",
    "if 'architecture_decision' in locals() and architecture_decision:\n",
    "    rec = architecture_decision['recommendation']\n",
    "    findings.append(f\"Optimal architecture determined: {rec}\")\n",
    "\n",
    "for i, finding in enumerate(findings, 1):\n",
    "    print(f\"   {i}. {finding}\")\n",
    "\n",
    "# Files generated\n",
    "print(f\"\\n📁 GENERATED FILES:\")\n",
    "output_files = [\n",
    "    'ast_only_analysis.csv',\n",
    "    'control_flow_analysis.csv', \n",
    "    'data_dependency_analysis.csv',\n",
    "    'architecture_decision.json'\n",
    "]\n",
    "\n",
    "for file_name in output_files:\n",
    "    file_path = results_dir / file_name\n",
    "    if file_path.exists():\n",
    "        size_kb = file_path.stat().st_size / 1024\n",
    "        print(f\"   ✅ {file_name} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"   ❌ {file_name} (not generated)\")\n",
    "\n",
    "# Validation of architecture decisions\n",
    "print(f\"\\n VALIDATION OF ARCHITECTURE DECISIONS:\")\n",
    "\n",
    "validations = []\n",
    "\n",
    "if 'architecture_decision' in locals() and architecture_decision:\n",
    "    rec = architecture_decision['recommendation']\n",
    "    if rec == \"AST-ONLY\":\n",
    "        validations.append(\"✅ AST-only architecture sufficient for this dataset\")\n",
    "    elif \"CFG\" in rec:\n",
    "        validations.append(\"✅ CFG analysis adds value to vulnerability detection\")\n",
    "    elif \"PDG\" in rec:\n",
    "        validations.append(\"✅ PDG analysis adds value to vulnerability detection\")\n",
    "    else:\n",
    "        validations.append(\"✅ Full analysis pipeline recommended\")\n",
    "\n",
    "if 'ast_analysis' in locals() and ast_analysis:\n",
    "    ast_df = pd.DataFrame(ast_analysis)\n",
    "    if len(ast_df) > 50:\n",
    "        validations.append(\"✅ Sufficient data for architecture analysis\")\n",
    "    else:\n",
    "        validations.append(\"⚠️ Limited data - results should be interpreted cautiously\")\n",
    "\n",
    "if 'architecture_decision' in locals() and architecture_decision:\n",
    "    effectiveness = architecture_decision['effectiveness']\n",
    "    if effectiveness > 80:\n",
    "        validations.append(\"✅ Recommended architecture achieves high effectiveness\")\n",
    "    else:\n",
    "        validations.append(\"⚠️ Recommended architecture has effectiveness limitations\")\n",
    "\n",
    "for validation in validations:\n",
    "    print(f\"   • {validation}\")\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(f\"   1. Implement recommended architecture in production\")\n",
    "print(f\"   2. Validate architecture performance on full dataset\")\n",
    "print(f\"   3. Proceed to Phase 5: Configuration Derivation\")\n",
    "print(f\"   4. Update project configuration with architecture decision\")\n",
    "\n",
    "print(f\"\\n PHASE 4 COMPLETE - ARCHITECTURE DECISION MADE!\")\n",
    "print(f\"   AST effectiveness: ANALYZED ✅\")\n",
    "print(f\"   CFG necessity: EVALUATED ✅\")\n",
    "print(f\"   PDG requirements: ASSESSED ✅\")\n",
    "print(f\"   Optimal architecture: DETERMINED ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
