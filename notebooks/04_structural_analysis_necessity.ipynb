{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5760b150",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Phase 4: Structural Analysis Necessity\n",
    "\n",
    "**Objective:** Determine if you actually need CFG/PDG or if AST suffices\n",
    "\n",
    "**Key Deliverables:**\n",
    "- AST-only classification effectiveness analysis\n",
    "- Control flow complexity assessment\n",
    "- Data dependency requirements evaluation\n",
    "- Architecture simplification opportunities\n",
    "- Evidence-based component selection\n",
    "\n",
    "**Building on Previous Phases:**\n",
    "- Phase 1: Dataset characteristics understood\n",
    "- Phase 2: Processing performance validated (0.004s avg)\n",
    "- Phase 3: Context-based approach proven superior\n",
    "- Now: Determine optimal architecture complexity\n",
    "\n",
    "**Scientific Question:** \n",
    "*Can AST patterns alone capture the vulnerability context, or do we need full CFG/PDG analysis?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656e525c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported processing modules\n",
      "üèóÔ∏è Phase 4 Setup complete!\n",
      "Project root: /Users/vernetemmanueladjobi/Desktop/KB_/vulnerability-kb\n",
      "Results directory: ../results\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Any\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project source to path\n",
    "project_root = Path('../')\n",
    "sys.path.append(str(project_root / 'src'))\n",
    "\n",
    "# Import processing modules\n",
    "try:\n",
    "    from extract_ast import extract_ast_patterns\n",
    "    from build_cfg import build_simple_cfg\n",
    "    from build_pdg import build_simple_pdg\n",
    "    print(\"‚úÖ Successfully imported processing modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "\n",
    "# Set up paths\n",
    "data_dir = project_root / 'data'\n",
    "raw_dir = data_dir / 'raw'\n",
    "results_dir = project_root / 'results'\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "\n",
    "print(\"üèóÔ∏è Phase 4 Setup complete!\")\n",
    "print(f\"Project root: {project_root.resolve()}\")\n",
    "print(f\"Results directory: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13169721",
   "metadata": {},
   "source": [
    "## 4.1 Load Previous Results and Vulnerability Data\n",
    "\n",
    "Load insights from Phases 1-3 to understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d0953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading previous phase results...\n",
      "‚úÖ Phase 1: 1217 CVEs\n",
      "‚úÖ Phase 2: 2317 performance samples\n",
      "‚úÖ Phase 3: Context analysis configuration loaded\n",
      "üìÇ Loading ALL vulnerability samples for structural analysis...\n",
      "‚úÖ Loaded 2317 vulnerability samples (100% of dataset)\n"
     ]
    }
   ],
   "source": [
    "# Load previous phase results\n",
    "print(\"üìÇ Loading previous phase results...\")\n",
    "\n",
    "# Load Phase 1 summary\n",
    "try:\n",
    "    with open(results_dir / 'vulrag_summary_report.json', 'r') as f:\n",
    "        phase1_summary = json.load(f)\n",
    "    print(f\"‚úÖ Phase 1: {phase1_summary['analysis_metadata']['total_cves']} CVEs\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Phase 1 summary not found\")\n",
    "    raise\n",
    "\n",
    "# Load Phase 2 performance results\n",
    "try:\n",
    "    phase2_df = pd.read_csv(results_dir / 'performance_analysis_results.csv')\n",
    "    print(f\"‚úÖ Phase 2: {len(phase2_df)} performance samples\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Phase 2 results not found\")\n",
    "    raise\n",
    "\n",
    "# Load Phase 3 context analysis\n",
    "try:\n",
    "    with open(results_dir / 'context_based_analysis_config.json', 'r') as f:\n",
    "        phase3_config = json.load(f)\n",
    "    print(f\"‚úÖ Phase 3: Context analysis configuration loaded\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Phase 3 configuration not found\")\n",
    "    raise\n",
    "\n",
    "# Modifier la fonction pour analyser TOUT le dataset\n",
    "def load_analysis_samples(sample_limit=None):  # None = tous les √©chantillons\n",
    "    \"\"\"Load ALL vulnerability samples for structural analysis\"\"\"\n",
    "    print(f\"üìÇ Loading ALL vulnerability samples for structural analysis...\")\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    # Get CWE files\n",
    "    cwe_files = list(raw_dir.glob(\"*.json\"))\n",
    "    \n",
    "    for cwe_file in sorted(cwe_files):\n",
    "        try:\n",
    "            with open(cwe_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            cwe = cwe_file.stem.split('_')[1]\n",
    "            \n",
    "            for cve_id, instances in data.items():\n",
    "                for idx, instance in enumerate(instances):\n",
    "                    code_before = instance.get('code_before_change', '')\n",
    "                    if code_before and len(code_before.strip()) > 20:\n",
    "                        samples.append({\n",
    "                            'cwe': cwe,\n",
    "                            'cve_id': cve_id,\n",
    "                            'instance_idx': idx,\n",
    "                            'code': code_before,\n",
    "                            'lines': len(code_before.split('\\n')),\n",
    "                            'chars': len(code_before)\n",
    "                        })\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {cwe_file.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(samples)} vulnerability samples (100% of dataset)\")\n",
    "    return samples\n",
    "\n",
    "# Load samples\n",
    "analysis_samples = load_analysis_samples(sample_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315db97",
   "metadata": {},
   "source": [
    "## 4.2 AST-Only Classification Effectiveness\n",
    "\n",
    "Test if AST patterns alone can detect the vulnerabilities found in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2437cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ast_only_effectiveness(samples):\n",
    "    \"\"\"Analyze how well AST patterns alone can detect vulnerabilities\"\"\"\n",
    "    print(\" ANALYZING AST-ONLY CLASSIFICATION EFFECTIVENESS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    ast_results = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"üìã Processing sample {i+1}/{len(samples)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Extract AST patterns - CORRECTION: use timeout_seconds\n",
    "            ast_result = extract_ast_patterns(sample['code'], timeout_seconds=10)\n",
    "            \n",
    "            if ast_result.get('success'):\n",
    "                patterns = ast_result.get('patterns', {})\n",
    "                \n",
    "                # Analyze what AST can detect\n",
    "                ast_detection = {\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'functions_detected': len(patterns.get('functions', [])),\n",
    "                    'calls_detected': len(patterns.get('calls', [])),\n",
    "                    'variables_detected': len(patterns.get('variables', [])),\n",
    "                    'pointers_detected': len(patterns.get('pointers', [])),\n",
    "                    'arrays_detected': len(patterns.get('arrays', [])),\n",
    "                    'conditionals_detected': len(patterns.get('conditions', [])),\n",
    "                    'loops_detected': len(patterns.get('loops', [])),\n",
    "                    'ast_success': True\n",
    "                }\n",
    "                \n",
    "                # Check for vulnerability indicators in AST\n",
    "                vulnerability_indicators = []\n",
    "                \n",
    "                # Buffer-related indicators\n",
    "                if any('buffer' in var.get('name', '').lower() for var in patterns.get('variables', [])):\n",
    "                    vulnerability_indicators.append('buffer_variables')\n",
    "                \n",
    "                # Unsafe function calls\n",
    "                unsafe_functions = ['strcpy', 'strcat', 'sprintf', 'gets', 'scanf', 'memcpy']\n",
    "                unsafe_calls = [call for call in patterns.get('calls', []) \n",
    "                              if call.get('function', '') in unsafe_functions]\n",
    "                if unsafe_calls:\n",
    "                    vulnerability_indicators.append('unsafe_functions')\n",
    "                \n",
    "                # Pointer operations\n",
    "                if patterns.get('pointers'):\n",
    "                    vulnerability_indicators.append('pointer_operations')\n",
    "                \n",
    "                # Array operations\n",
    "                if patterns.get('arrays'):\n",
    "                    vulnerability_indicators.append('array_operations')\n",
    "                \n",
    "                ast_detection['vulnerability_indicators'] = vulnerability_indicators\n",
    "                ast_detection['indicator_count'] = len(vulnerability_indicators)\n",
    "                \n",
    "            else:\n",
    "                ast_detection = {\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'ast_success': False,\n",
    "                    'vulnerability_indicators': [],\n",
    "                    'indicator_count': 0\n",
    "                }\n",
    "            \n",
    "            ast_results.append(ast_detection)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error analyzing sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return ast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9cc1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing AST-only vulnerability detection...\n",
      " ANALYZING AST-ONLY CLASSIFICATION EFFECTIVENESS\n",
      "============================================================\n",
      "üìã Processing sample 1/2317...\n",
      "üìã Processing sample 21/2317...\n",
      "üìã Processing sample 41/2317...\n",
      "üìã Processing sample 61/2317...\n",
      "üìã Processing sample 81/2317...\n",
      "üìã Processing sample 101/2317...\n",
      "üìã Processing sample 121/2317...\n",
      "üìã Processing sample 141/2317...\n",
      "üìã Processing sample 161/2317...\n",
      "üìã Processing sample 181/2317...\n",
      "üìã Processing sample 201/2317...\n",
      "üìã Processing sample 221/2317...\n",
      "üìã Processing sample 241/2317...\n",
      "üìã Processing sample 261/2317...\n",
      "üìã Processing sample 281/2317...\n",
      "üìã Processing sample 301/2317...\n",
      "üìã Processing sample 321/2317...\n",
      "üìã Processing sample 341/2317...\n",
      "üìã Processing sample 361/2317...\n",
      "üìã Processing sample 381/2317...\n",
      "üìã Processing sample 401/2317...\n",
      "üìã Processing sample 421/2317...\n",
      "üìã Processing sample 441/2317...\n",
      "üìã Processing sample 461/2317...\n",
      "üìã Processing sample 481/2317...\n",
      "üìã Processing sample 501/2317...\n",
      "üìã Processing sample 521/2317...\n",
      "üìã Processing sample 541/2317...\n",
      "üìã Processing sample 561/2317...\n",
      "üìã Processing sample 581/2317...\n",
      "üìã Processing sample 601/2317...\n",
      "üìã Processing sample 621/2317...\n",
      "üìã Processing sample 641/2317...\n",
      "üìã Processing sample 661/2317...\n",
      "üìã Processing sample 681/2317...\n",
      "üìã Processing sample 701/2317...\n",
      "üìã Processing sample 721/2317...\n",
      "üìã Processing sample 741/2317...\n",
      "üìã Processing sample 761/2317...\n",
      "üìã Processing sample 781/2317...\n",
      "üìã Processing sample 801/2317...\n",
      "üìã Processing sample 821/2317...\n",
      "üìã Processing sample 841/2317...\n",
      "üìã Processing sample 861/2317...\n",
      "üìã Processing sample 881/2317...\n",
      "üìã Processing sample 901/2317...\n",
      "üìã Processing sample 921/2317...\n",
      "üìã Processing sample 941/2317...\n",
      "üìã Processing sample 961/2317...\n",
      "üìã Processing sample 981/2317...\n",
      "üìã Processing sample 1001/2317...\n",
      "üìã Processing sample 1021/2317...\n",
      "üìã Processing sample 1041/2317...\n",
      "üìã Processing sample 1061/2317...\n",
      "üìã Processing sample 1081/2317...\n",
      "üìã Processing sample 1101/2317...\n",
      "üìã Processing sample 1121/2317...\n",
      "üìã Processing sample 1141/2317...\n",
      "üìã Processing sample 1161/2317...\n",
      "üìã Processing sample 1181/2317...\n",
      "üìã Processing sample 1201/2317...\n",
      "üìã Processing sample 1221/2317...\n",
      "üìã Processing sample 1241/2317...\n",
      "üìã Processing sample 1261/2317...\n",
      "üìã Processing sample 1281/2317...\n",
      "üìã Processing sample 1301/2317...\n",
      "üìã Processing sample 1321/2317...\n",
      "üìã Processing sample 1341/2317...\n",
      "üìã Processing sample 1361/2317...\n",
      "üìã Processing sample 1381/2317...\n",
      "üìã Processing sample 1401/2317...\n",
      "üìã Processing sample 1421/2317...\n",
      "üìã Processing sample 1441/2317...\n",
      "üìã Processing sample 1461/2317...\n",
      "üìã Processing sample 1481/2317...\n",
      "üìã Processing sample 1501/2317...\n",
      "üìã Processing sample 1521/2317...\n",
      "üìã Processing sample 1541/2317...\n",
      "üìã Processing sample 1561/2317...\n",
      "üìã Processing sample 1581/2317...\n",
      "üìã Processing sample 1601/2317...\n",
      "üìã Processing sample 1621/2317...\n",
      "üìã Processing sample 1641/2317...\n",
      "üìã Processing sample 1661/2317...\n",
      "üìã Processing sample 1681/2317...\n",
      "üìã Processing sample 1701/2317...\n",
      "üìã Processing sample 1721/2317...\n",
      "üìã Processing sample 1741/2317...\n",
      "üìã Processing sample 1761/2317...\n",
      "üìã Processing sample 1781/2317...\n",
      "üìã Processing sample 1801/2317...\n",
      "üìã Processing sample 1821/2317...\n",
      "üìã Processing sample 1841/2317...\n",
      "üìã Processing sample 1861/2317...\n",
      "üìã Processing sample 1881/2317...\n",
      "üìã Processing sample 1901/2317...\n",
      "üìã Processing sample 1921/2317...\n",
      "üìã Processing sample 1941/2317...\n",
      "üìã Processing sample 1961/2317...\n",
      "üìã Processing sample 1981/2317...\n",
      "üìã Processing sample 2001/2317...\n",
      "üìã Processing sample 2021/2317...\n",
      "üìã Processing sample 2041/2317...\n",
      "üìã Processing sample 2061/2317...\n",
      "üìã Processing sample 2081/2317...\n",
      "üìã Processing sample 2101/2317...\n",
      "üìã Processing sample 2121/2317...\n",
      "üìã Processing sample 2141/2317...\n",
      "üìã Processing sample 2161/2317...\n",
      "üìã Processing sample 2181/2317...\n",
      "üìã Processing sample 2201/2317...\n",
      "üìã Processing sample 2221/2317...\n",
      "üìã Processing sample 2241/2317...\n",
      "üìã Processing sample 2261/2317...\n",
      "üìã Processing sample 2281/2317...\n",
      "üìã Processing sample 2301/2317...\n",
      "\n",
      "üìä AST-ONLY ANALYSIS RESULTS:\n",
      "   ‚Ä¢ Total samples: 2317\n",
      "   ‚Ä¢ AST extraction success: 2317/2317 (100.0%)\n",
      "   ‚Ä¢ Vulnerability indicators detected: 2249/2317 (97.1%)\n",
      "   ‚Ä¢ Most common indicators:\n",
      "      ‚Ä¢ pointer_operations: 2238/2317 (96.6%)\n",
      "      ‚Ä¢ array_operations: 725/2317 (31.3%)\n",
      "      ‚Ä¢ unsafe_functions: 226/2317 (9.8%)\n",
      "\n",
      " AST DETECTION BY CWE TYPE:\n",
      "   ‚Ä¢ CWE-119: 166/173 (96.0%)\n",
      "   ‚Ä¢ CWE-125: 138/140 (98.6%)\n",
      "   ‚Ä¢ CWE-200: 141/153 (92.2%)\n",
      "   ‚Ä¢ CWE-20: 178/182 (97.8%)\n",
      "   ‚Ä¢ CWE-264: 116/120 (96.7%)\n",
      "   ‚Ä¢ CWE-362: 314/320 (98.1%)\n",
      "   ‚Ä¢ CWE-401: 97/101 (96.0%)\n",
      "   ‚Ä¢ CWE-416: 647/660 (98.0%)\n",
      "   ‚Ä¢ CWE-476: 269/281 (95.7%)\n",
      "   ‚Ä¢ CWE-787: 183/187 (97.9%)\n",
      "\n",
      "üíæ AST-only analysis saved to: ../results/ast_only_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Analyze AST-only effectiveness\n",
    "print(\" Testing AST-only vulnerability detection...\")\n",
    "ast_analysis = analyze_ast_only_effectiveness(analysis_samples)\n",
    "\n",
    "# Analyze results\n",
    "if ast_analysis:\n",
    "    ast_df = pd.DataFrame(ast_analysis)\n",
    "    successful_ast = ast_df[ast_df['ast_success']]\n",
    "    \n",
    "    print(f\"\\nüìä AST-ONLY ANALYSIS RESULTS:\")\n",
    "    print(f\"   ‚Ä¢ Total samples: {len(ast_df)}\")\n",
    "    print(f\"   ‚Ä¢ AST extraction success: {len(successful_ast)}/{len(ast_df)} ({len(successful_ast)/len(ast_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_ast) > 0:\n",
    "        # Vulnerability detection capability\n",
    "        samples_with_indicators = successful_ast[successful_ast['indicator_count'] > 0]\n",
    "        detection_rate = len(samples_with_indicators) / len(successful_ast) * 100\n",
    "        \n",
    "        print(f\"   ‚Ä¢ Vulnerability indicators detected: {len(samples_with_indicators)}/{len(successful_ast)} ({detection_rate:.1f}%)\")\n",
    "        \n",
    "        # Most common indicators\n",
    "        all_indicators = []\n",
    "        for indicators in successful_ast['vulnerability_indicators']:\n",
    "            all_indicators.extend(indicators)\n",
    "        \n",
    "        indicator_counter = Counter(all_indicators)\n",
    "        print(f\"   ‚Ä¢ Most common indicators:\")\n",
    "        for indicator, count in indicator_counter.most_common(3):\n",
    "            percentage = (count / len(successful_ast)) * 100\n",
    "            print(f\"      ‚Ä¢ {indicator}: {count}/{len(successful_ast)} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # CWE-specific analysis\n",
    "        print(f\"\\n AST DETECTION BY CWE TYPE:\")\n",
    "        for cwe in successful_ast['cwe'].unique():\n",
    "            cwe_data = successful_ast[successful_ast['cwe'] == cwe]\n",
    "            cwe_indicators = cwe_data[cwe_data['indicator_count'] > 0]\n",
    "            cwe_detection_rate = len(cwe_indicators) / len(cwe_data) * 100\n",
    "            \n",
    "            print(f\"   ‚Ä¢ {cwe}: {len(cwe_indicators)}/{len(cwe_data)} ({cwe_detection_rate:.1f}%)\")\n",
    "    \n",
    "    # Save results\n",
    "    ast_df.to_csv(results_dir / 'ast_only_analysis.csv', index=False)\n",
    "    print(f\"\\nüíæ AST-only analysis saved to: {results_dir / 'ast_only_analysis.csv'}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No AST analysis results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3c7eca",
   "metadata": {},
   "source": [
    "## 4.3 Control Flow Complexity Analysis\n",
    "\n",
    "Analyze how many functions have complex control flow that might require CFG analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "418c6c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing control flow complexity...\n",
      "üîÑ ANALYZING CONTROL FLOW COMPLEXITY\n",
      "============================================================\n",
      "üìã Processing sample 1/2317...\n",
      "üìã Processing sample 21/2317...\n",
      "üìã Processing sample 41/2317...\n",
      "üìã Processing sample 61/2317...\n",
      "üìã Processing sample 81/2317...\n",
      "üìã Processing sample 101/2317...\n",
      "üìã Processing sample 121/2317...\n",
      "üìã Processing sample 141/2317...\n",
      "üìã Processing sample 161/2317...\n",
      "üìã Processing sample 181/2317...\n",
      "üìã Processing sample 201/2317...\n",
      "üìã Processing sample 221/2317...\n",
      "üìã Processing sample 241/2317...\n",
      "üìã Processing sample 261/2317...\n",
      "üìã Processing sample 281/2317...\n",
      "üìã Processing sample 301/2317...\n",
      "üìã Processing sample 321/2317...\n",
      "üìã Processing sample 341/2317...\n",
      "üìã Processing sample 361/2317...\n",
      "üìã Processing sample 381/2317...\n",
      "üìã Processing sample 401/2317...\n",
      "üìã Processing sample 421/2317...\n",
      "üìã Processing sample 441/2317...\n",
      "üìã Processing sample 461/2317...\n",
      "üìã Processing sample 481/2317...\n",
      "üìã Processing sample 501/2317...\n",
      "üìã Processing sample 521/2317...\n",
      "üìã Processing sample 541/2317...\n",
      "üìã Processing sample 561/2317...\n",
      "üìã Processing sample 581/2317...\n",
      "üìã Processing sample 601/2317...\n",
      "üìã Processing sample 621/2317...\n",
      "üìã Processing sample 641/2317...\n",
      "üìã Processing sample 661/2317...\n",
      "üìã Processing sample 681/2317...\n",
      "üìã Processing sample 701/2317...\n",
      "üìã Processing sample 721/2317...\n",
      "üìã Processing sample 741/2317...\n",
      "üìã Processing sample 761/2317...\n",
      "üìã Processing sample 781/2317...\n",
      "üìã Processing sample 801/2317...\n",
      "üìã Processing sample 821/2317...\n",
      "üìã Processing sample 841/2317...\n",
      "üìã Processing sample 861/2317...\n",
      "üìã Processing sample 881/2317...\n",
      "üìã Processing sample 901/2317...\n",
      "üìã Processing sample 921/2317...\n",
      "üìã Processing sample 941/2317...\n",
      "üìã Processing sample 961/2317...\n",
      "üìã Processing sample 981/2317...\n",
      "üìã Processing sample 1001/2317...\n",
      "üìã Processing sample 1021/2317...\n",
      "üìã Processing sample 1041/2317...\n",
      "üìã Processing sample 1061/2317...\n",
      "üìã Processing sample 1081/2317...\n",
      "üìã Processing sample 1101/2317...\n",
      "üìã Processing sample 1121/2317...\n",
      "üìã Processing sample 1141/2317...\n",
      "üìã Processing sample 1161/2317...\n",
      "üìã Processing sample 1181/2317...\n",
      "üìã Processing sample 1201/2317...\n",
      "üìã Processing sample 1221/2317...\n",
      "üìã Processing sample 1241/2317...\n",
      "üìã Processing sample 1261/2317...\n",
      "üìã Processing sample 1281/2317...\n",
      "üìã Processing sample 1301/2317...\n",
      "üìã Processing sample 1321/2317...\n",
      "üìã Processing sample 1341/2317...\n",
      "üìã Processing sample 1361/2317...\n",
      "üìã Processing sample 1381/2317...\n",
      "üìã Processing sample 1401/2317...\n",
      "üìã Processing sample 1421/2317...\n",
      "üìã Processing sample 1441/2317...\n",
      "üìã Processing sample 1461/2317...\n",
      "üìã Processing sample 1481/2317...\n",
      "üìã Processing sample 1501/2317...\n",
      "üìã Processing sample 1521/2317...\n",
      "üìã Processing sample 1541/2317...\n",
      "üìã Processing sample 1561/2317...\n",
      "üìã Processing sample 1581/2317...\n",
      "üìã Processing sample 1601/2317...\n",
      "üìã Processing sample 1621/2317...\n",
      "üìã Processing sample 1641/2317...\n",
      "üìã Processing sample 1661/2317...\n",
      "üìã Processing sample 1681/2317...\n",
      "üìã Processing sample 1701/2317...\n",
      "üìã Processing sample 1721/2317...\n",
      "üìã Processing sample 1741/2317...\n",
      "üìã Processing sample 1761/2317...\n",
      "üìã Processing sample 1781/2317...\n",
      "üìã Processing sample 1801/2317...\n",
      "üìã Processing sample 1821/2317...\n",
      "üìã Processing sample 1841/2317...\n",
      "üìã Processing sample 1861/2317...\n",
      "üìã Processing sample 1881/2317...\n",
      "üìã Processing sample 1901/2317...\n",
      "üìã Processing sample 1921/2317...\n",
      "üìã Processing sample 1941/2317...\n",
      "üìã Processing sample 1961/2317...\n",
      "üìã Processing sample 1981/2317...\n",
      "üìã Processing sample 2001/2317...\n",
      "üìã Processing sample 2021/2317...\n",
      "üìã Processing sample 2041/2317...\n",
      "üìã Processing sample 2061/2317...\n",
      "üìã Processing sample 2081/2317...\n",
      "üìã Processing sample 2101/2317...\n",
      "üìã Processing sample 2121/2317...\n",
      "üìã Processing sample 2141/2317...\n",
      "üìã Processing sample 2161/2317...\n",
      "üìã Processing sample 2181/2317...\n",
      "üìã Processing sample 2201/2317...\n",
      "üìã Processing sample 2221/2317...\n",
      "üìã Processing sample 2241/2317...\n",
      "üìã Processing sample 2261/2317...\n",
      "üìã Processing sample 2281/2317...\n",
      "üìã Processing sample 2301/2317...\n",
      "\n",
      "üìä CONTROL FLOW COMPLEXITY RESULTS:\n",
      "   ‚Ä¢ Total samples: 2317\n",
      "   ‚Ä¢ CFG construction success: 2317/2317 (100.0%)\n",
      "\n",
      " COMPLEXITY STATISTICS:\n",
      "   ‚Ä¢ Mean complexity: 1.00\n",
      "   ‚Ä¢ Median complexity: 1.00\n",
      "   ‚Ä¢ 95th percentile: 1.00\n",
      "   ‚Ä¢ Max complexity: 1.00\n",
      "   ‚Ä¢ Min complexity: 0.00\n",
      "\n",
      " COMPLEXITY DISTRIBUTION:\n",
      "   ‚Ä¢ Simple (‚â§3): 2317/2317 (100.0%)\n",
      "   ‚Ä¢ Moderate (4-10): 0/2317 (0.0%)\n",
      "   ‚Ä¢ Complex (>10): 0/2317 (0.0%)\n",
      "\n",
      "üîç COMPLEXITY BY CWE TYPE:\n",
      "   ‚Ä¢ CWE-119: avg 1.0, 0/173 complex (0.0%)\n",
      "   ‚Ä¢ CWE-125: avg 1.0, 0/140 complex (0.0%)\n",
      "   ‚Ä¢ CWE-200: avg 1.0, 0/153 complex (0.0%)\n",
      "   ‚Ä¢ CWE-20: avg 1.0, 0/182 complex (0.0%)\n",
      "   ‚Ä¢ CWE-264: avg 1.0, 0/120 complex (0.0%)\n",
      "   ‚Ä¢ CWE-362: avg 1.0, 0/320 complex (0.0%)\n",
      "   ‚Ä¢ CWE-401: avg 1.0, 0/101 complex (0.0%)\n",
      "   ‚Ä¢ CWE-416: avg 1.0, 0/660 complex (0.0%)\n",
      "   ‚Ä¢ CWE-476: avg 1.0, 0/281 complex (0.0%)\n",
      "   ‚Ä¢ CWE-787: avg 1.0, 0/187 complex (0.0%)\n",
      "\n",
      "üéØ CFG NECESSITY ANALYSIS:\n",
      "   ‚Ä¢ Samples requiring CFG analysis: 0/2317 (0.0%)\n",
      "   ‚Ä¢ RECOMMENDATION: AST-only may be sufficient for most cases\n",
      "\n",
      "üíæ Control flow analysis saved to: ../results/control_flow_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "def analyze_control_flow_complexity(samples):\n",
    "    \"\"\"Analyze control flow complexity to determine CFG necessity\"\"\"\n",
    "    print(\"üîÑ ANALYZING CONTROL FLOW COMPLEXITY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    cf_analysis = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"üìã Processing sample {i+1}/{len(samples)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Build CFG - CORRECTION: use build_simple_cfg with timeout_seconds\n",
    "            cfg_result = build_simple_cfg(sample['code'], timeout_seconds=10)\n",
    "            \n",
    "            if cfg_result.get('success'):\n",
    "                # Extract complexity from global stats\n",
    "                global_stats = cfg_result.get('global_stats', {})\n",
    "                total_nodes = global_stats.get('total_nodes', 0)\n",
    "                total_edges = global_stats.get('total_edges', 0)\n",
    "                \n",
    "                # Calculate cyclomatic complexity (edges - nodes + 2)\n",
    "                cyclomatic_complexity = total_edges - total_nodes + 2 if total_nodes > 0 else 1\n",
    "                \n",
    "                cf_analysis.append({\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'cfg_nodes': total_nodes,\n",
    "                    'cfg_edges': total_edges,\n",
    "                    'cyclomatic_complexity': cyclomatic_complexity,\n",
    "                    'cfg_success': True\n",
    "                })\n",
    "            else:\n",
    "                cf_analysis.append({\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'cfg_nodes': 0,\n",
    "                    'cfg_edges': 0,\n",
    "                    'cyclomatic_complexity': 0,\n",
    "                    'cfg_success': False\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error analyzing CFG for sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return cf_analysis\n",
    "\n",
    "# Analyze control flow complexity\n",
    "print(\"üîÑ Testing control flow complexity...\")\n",
    "cf_analysis = analyze_control_flow_complexity(analysis_samples)\n",
    "\n",
    "# Analyze results\n",
    "if cf_analysis:\n",
    "    cf_df = pd.DataFrame(cf_analysis)\n",
    "    successful_cfg = cf_df[cf_df['cfg_success']]\n",
    "    \n",
    "    print(f\"\\nüìä CONTROL FLOW COMPLEXITY RESULTS:\")\n",
    "    print(f\"   ‚Ä¢ Total samples: {len(cf_df)}\")\n",
    "    print(f\"   ‚Ä¢ CFG construction success: {len(successful_cfg)}/{len(cf_df)} ({len(successful_cfg)/len(cf_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_cfg) > 0:\n",
    "        # Complexity distribution - CORRECTION: Handle percentiles safely\n",
    "        complexity_values = successful_cfg['cyclomatic_complexity'].values\n",
    "        \n",
    "        print(f\"\\n COMPLEXITY STATISTICS:\")\n",
    "        print(f\"   ‚Ä¢ Mean complexity: {complexity_values.mean():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Median complexity: {np.median(complexity_values):.2f}\")\n",
    "        \n",
    "        # Calculate 95th percentile safely\n",
    "        if len(complexity_values) > 1:\n",
    "            percentile_95 = np.percentile(complexity_values, 95)\n",
    "            print(f\"   ‚Ä¢ 95th percentile: {percentile_95:.2f}\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ 95th percentile: {complexity_values[0]:.2f} (single value)\")\n",
    "            \n",
    "        print(f\"   ‚Ä¢ Max complexity: {complexity_values.max():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Min complexity: {complexity_values.min():.2f}\")\n",
    "        \n",
    "        # Complexity categories\n",
    "        simple_cfg = successful_cfg[successful_cfg['cyclomatic_complexity'] <= 3]\n",
    "        moderate_cfg = successful_cfg[(successful_cfg['cyclomatic_complexity'] > 3) & \n",
    "                                    (successful_cfg['cyclomatic_complexity'] <= 10)]\n",
    "        complex_cfg = successful_cfg[successful_cfg['cyclomatic_complexity'] > 10]\n",
    "        \n",
    "        print(f\"\\n COMPLEXITY DISTRIBUTION:\")\n",
    "        print(f\"   ‚Ä¢ Simple (‚â§3): {len(simple_cfg)}/{len(successful_cfg)} ({len(simple_cfg)/len(successful_cfg)*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Moderate (4-10): {len(moderate_cfg)}/{len(successful_cfg)} ({len(moderate_cfg)/len(successful_cfg)*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Complex (>10): {len(complex_cfg)}/{len(successful_cfg)} ({len(complex_cfg)/len(successful_cfg)*100:.1f}%)\")\n",
    "        \n",
    "        # CWE-specific complexity\n",
    "        print(f\"\\nüîç COMPLEXITY BY CWE TYPE:\")\n",
    "        for cwe in successful_cfg['cwe'].unique():\n",
    "            cwe_data = successful_cfg[successful_cfg['cwe'] == cwe]\n",
    "            avg_complexity = cwe_data['cyclomatic_complexity'].mean()\n",
    "            complex_count = len(cwe_data[cwe_data['cyclomatic_complexity'] > 10])\n",
    "            complex_rate = complex_count / len(cwe_data) * 100\n",
    "            \n",
    "            print(f\"   ‚Ä¢ {cwe}: avg {avg_complexity:.1f}, {complex_count}/{len(cwe_data)} complex ({complex_rate:.1f}%)\")\n",
    "        \n",
    "        # Determine CFG necessity\n",
    "        complex_threshold = 10  # Arbitrary threshold for \"complex\" control flow\n",
    "        complex_samples = successful_cfg[successful_cfg['cyclomatic_complexity'] > complex_threshold]\n",
    "        cfg_necessity_rate = len(complex_samples) / len(successful_cfg) * 100\n",
    "        \n",
    "        print(f\"\\nüéØ CFG NECESSITY ANALYSIS:\")\n",
    "        print(f\"   ‚Ä¢ Samples requiring CFG analysis: {len(complex_samples)}/{len(successful_cfg)} ({cfg_necessity_rate:.1f}%)\")\n",
    "        \n",
    "        if cfg_necessity_rate > 50:\n",
    "            print(\"   ‚Ä¢ RECOMMENDATION: CFG analysis is important for this dataset\")\n",
    "        elif cfg_necessity_rate > 20:\n",
    "            print(\"   ‚Ä¢ RECOMMENDATION: CFG analysis provides moderate value\")\n",
    "        else:\n",
    "            print(\"   ‚Ä¢ RECOMMENDATION: AST-only may be sufficient for most cases\")\n",
    "    \n",
    "    # Save results\n",
    "    cf_df.to_csv(results_dir / 'control_flow_analysis.csv', index=False)\n",
    "    print(f\"\\nüíæ Control flow analysis saved to: {results_dir / 'control_flow_analysis.csv'}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No control flow analysis results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e07584",
   "metadata": {},
   "source": [
    "## 4.4 Data Dependency Requirements\n",
    "\n",
    "Analyze if vulnerabilities require data flow analysis (PDG) or if simpler approaches suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc8e0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Testing data dependency requirements...\n",
      " ANALYZING DATA DEPENDENCY REQUIREMENTS\n",
      "============================================================\n",
      "üìã Processing sample 1/2317...\n",
      "üìã Processing sample 21/2317...\n",
      "üìã Processing sample 41/2317...\n",
      "üìã Processing sample 61/2317...\n",
      "üìã Processing sample 81/2317...\n",
      "üìã Processing sample 101/2317...\n",
      "üìã Processing sample 121/2317...\n",
      "üìã Processing sample 141/2317...\n",
      "üìã Processing sample 161/2317...\n",
      "üìã Processing sample 181/2317...\n",
      "üìã Processing sample 201/2317...\n",
      "üìã Processing sample 221/2317...\n",
      "üìã Processing sample 241/2317...\n",
      "üìã Processing sample 261/2317...\n",
      "üìã Processing sample 281/2317...\n",
      "üìã Processing sample 301/2317...\n",
      "üìã Processing sample 321/2317...\n",
      "üìã Processing sample 341/2317...\n",
      "üìã Processing sample 361/2317...\n",
      "üìã Processing sample 381/2317...\n",
      "üìã Processing sample 401/2317...\n",
      "üìã Processing sample 421/2317...\n",
      "üìã Processing sample 441/2317...\n",
      "üìã Processing sample 461/2317...\n",
      "üìã Processing sample 481/2317...\n",
      "üìã Processing sample 501/2317...\n",
      "üìã Processing sample 521/2317...\n",
      "üìã Processing sample 541/2317...\n",
      "üìã Processing sample 561/2317...\n",
      "üìã Processing sample 581/2317...\n",
      "üìã Processing sample 601/2317...\n",
      "üìã Processing sample 621/2317...\n",
      "üìã Processing sample 641/2317...\n",
      "üìã Processing sample 661/2317...\n",
      "üìã Processing sample 681/2317...\n",
      "üìã Processing sample 701/2317...\n",
      "üìã Processing sample 721/2317...\n",
      "üìã Processing sample 741/2317...\n",
      "üìã Processing sample 761/2317...\n",
      "üìã Processing sample 781/2317...\n",
      "üìã Processing sample 801/2317...\n",
      "üìã Processing sample 821/2317...\n",
      "üìã Processing sample 841/2317...\n",
      "üìã Processing sample 861/2317...\n",
      "üìã Processing sample 881/2317...\n",
      "üìã Processing sample 901/2317...\n",
      "üìã Processing sample 921/2317...\n",
      "üìã Processing sample 941/2317...\n",
      "üìã Processing sample 961/2317...\n",
      "üìã Processing sample 981/2317...\n",
      "üìã Processing sample 1001/2317...\n",
      "üìã Processing sample 1021/2317...\n",
      "üìã Processing sample 1041/2317...\n",
      "üìã Processing sample 1061/2317...\n",
      "üìã Processing sample 1081/2317...\n",
      "üìã Processing sample 1101/2317...\n",
      "üìã Processing sample 1121/2317...\n",
      "üìã Processing sample 1141/2317...\n",
      "üìã Processing sample 1161/2317...\n",
      "üìã Processing sample 1181/2317...\n",
      "üìã Processing sample 1201/2317...\n",
      "üìã Processing sample 1221/2317...\n",
      "üìã Processing sample 1241/2317...\n",
      "üìã Processing sample 1261/2317...\n",
      "üìã Processing sample 1281/2317...\n",
      "üìã Processing sample 1301/2317...\n",
      "üìã Processing sample 1321/2317...\n",
      "üìã Processing sample 1341/2317...\n",
      "üìã Processing sample 1361/2317...\n",
      "üìã Processing sample 1381/2317...\n",
      "üìã Processing sample 1401/2317...\n",
      "üìã Processing sample 1421/2317...\n",
      "üìã Processing sample 1441/2317...\n",
      "üìã Processing sample 1461/2317...\n",
      "üìã Processing sample 1481/2317...\n",
      "üìã Processing sample 1501/2317...\n",
      "üìã Processing sample 1521/2317...\n",
      "üìã Processing sample 1541/2317...\n",
      "üìã Processing sample 1561/2317...\n",
      "üìã Processing sample 1581/2317...\n",
      "üìã Processing sample 1601/2317...\n",
      "üìã Processing sample 1621/2317...\n",
      "üìã Processing sample 1641/2317...\n",
      "üìã Processing sample 1661/2317...\n",
      "üìã Processing sample 1681/2317...\n",
      "üìã Processing sample 1701/2317...\n",
      "üìã Processing sample 1721/2317...\n",
      "üìã Processing sample 1741/2317...\n",
      "üìã Processing sample 1761/2317...\n",
      "üìã Processing sample 1781/2317...\n",
      "üìã Processing sample 1801/2317...\n",
      "üìã Processing sample 1821/2317...\n",
      "üìã Processing sample 1841/2317...\n",
      "üìã Processing sample 1861/2317...\n",
      "üìã Processing sample 1881/2317...\n",
      "üìã Processing sample 1901/2317...\n",
      "üìã Processing sample 1921/2317...\n",
      "üìã Processing sample 1941/2317...\n",
      "üìã Processing sample 1961/2317...\n",
      "üìã Processing sample 1981/2317...\n",
      "üìã Processing sample 2001/2317...\n",
      "üìã Processing sample 2021/2317...\n",
      "üìã Processing sample 2041/2317...\n",
      "üìã Processing sample 2061/2317...\n",
      "üìã Processing sample 2081/2317...\n",
      "üìã Processing sample 2101/2317...\n",
      "üìã Processing sample 2121/2317...\n",
      "üìã Processing sample 2141/2317...\n",
      "üìã Processing sample 2161/2317...\n",
      "üìã Processing sample 2181/2317...\n",
      "üìã Processing sample 2201/2317...\n",
      "üìã Processing sample 2221/2317...\n",
      "üìã Processing sample 2241/2317...\n",
      "üìã Processing sample 2261/2317...\n",
      "üìã Processing sample 2281/2317...\n",
      "üìã Processing sample 2301/2317...\n",
      "\n",
      "üìä DATA DEPENDENCY ANALYSIS RESULTS:\n",
      "   ‚Ä¢ Total samples: 2317\n",
      "   ‚Ä¢ PDG construction success: 2317/2317 (100.0%)\n",
      "\n",
      " DEPENDENCY STATISTICS:\n",
      "   ‚Ä¢ Mean dependencies: 44.60\n",
      "   ‚Ä¢ Median dependencies: 15.00\n",
      "   ‚Ä¢ 95th percentile: 166.00\n",
      "   ‚Ä¢ Max dependencies: 2311.00\n",
      "   ‚Ä¢ Min dependencies: 0.00\n",
      "\n",
      "üìä DATA FLOW REQUIREMENTS:\n",
      "   ‚Ä¢ Samples with data dependencies: 1656/2317 (71.5%)\n",
      "\n",
      "üîç DATA FLOW BY CWE TYPE:\n",
      "   ‚Ä¢ CWE-119: 131/173 with flow (75.7%), avg 53.6 deps\n",
      "   ‚Ä¢ CWE-125: 110/140 with flow (78.6%), avg 60.7 deps\n",
      "   ‚Ä¢ CWE-200: 107/153 with flow (69.9%), avg 38.3 deps\n",
      "   ‚Ä¢ CWE-20: 141/182 with flow (77.5%), avg 52.8 deps\n",
      "   ‚Ä¢ CWE-264: 81/120 with flow (67.5%), avg 31.1 deps\n",
      "   ‚Ä¢ CWE-362: 216/320 with flow (67.5%), avg 41.9 deps\n",
      "   ‚Ä¢ CWE-401: 69/101 with flow (68.3%), avg 38.4 deps\n",
      "   ‚Ä¢ CWE-416: 451/660 with flow (68.3%), avg 37.2 deps\n",
      "   ‚Ä¢ CWE-476: 200/281 with flow (71.2%), avg 44.8 deps\n",
      "   ‚Ä¢ CWE-787: 150/187 with flow (80.2%), avg 64.0 deps\n",
      "\n",
      "üéØ PDG NECESSITY ANALYSIS:\n",
      "   ‚Ä¢ Samples requiring PDG analysis: 1535/2317 (66.2%)\n",
      "   ‚Ä¢ RECOMMENDATION: PDG analysis is important for this dataset\n",
      "\n",
      "üíæ Data dependency analysis saved to: ../results/data_dependency_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_dependency_requirements(samples):\n",
    "    \"\"\"Analyze data dependency requirements to determine PDG necessity\"\"\"\n",
    "    print(\" ANALYZING DATA DEPENDENCY REQUIREMENTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    pdg_analysis = []\n",
    "    \n",
    "    for i, sample in enumerate(samples):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"üìã Processing sample {i+1}/{len(samples)}...\")\n",
    "        \n",
    "        try:\n",
    "            # Build PDG - CORRECTION: use build_simple_pdg with timeout_seconds\n",
    "            pdg_result = build_simple_pdg(sample['code'], timeout_seconds=10)\n",
    "            \n",
    "            if pdg_result.get('success'):\n",
    "                # Extract dependency count from global stats\n",
    "                global_stats = pdg_result.get('global_stats', {})\n",
    "                dependency_count = global_stats.get('total_dependencies', 0)\n",
    "                \n",
    "                pdg_analysis.append({\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'dependency_count': dependency_count,\n",
    "                    'pdg_success': True,\n",
    "                    'has_data_flow': dependency_count > 0\n",
    "                })\n",
    "            else:\n",
    "                pdg_analysis.append({\n",
    "                    'cwe': sample['cwe'],\n",
    "                    'cve_id': sample['cve_id'],\n",
    "                    'lines': sample['lines'],\n",
    "                    'chars': sample['chars'],\n",
    "                    'dependency_count': 0,\n",
    "                    'pdg_success': False,\n",
    "                    'has_data_flow': False\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error analyzing PDG for sample {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return pdg_analysis\n",
    "\n",
    "# Analyze data dependencies\n",
    "print(\"üîó Testing data dependency requirements...\")\n",
    "pdg_analysis = analyze_data_dependency_requirements(analysis_samples)\n",
    "\n",
    "# Analyze results\n",
    "if pdg_analysis:\n",
    "    pdg_df = pd.DataFrame(pdg_analysis)\n",
    "    successful_pdg = pdg_df[pdg_df['pdg_success']]\n",
    "    \n",
    "    print(f\"\\nüìä DATA DEPENDENCY ANALYSIS RESULTS:\")\n",
    "    print(f\"   ‚Ä¢ Total samples: {len(pdg_df)}\")\n",
    "    print(f\"   ‚Ä¢ PDG construction success: {len(successful_pdg)}/{len(pdg_df)} ({len(successful_pdg)/len(pdg_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_pdg) > 0:\n",
    "        # Dependency statistics - CORRECTION: Handle percentiles safely\n",
    "        dependency_values = successful_pdg['dependency_count'].values\n",
    "        \n",
    "        print(f\"\\n DEPENDENCY STATISTICS:\")\n",
    "        print(f\"   ‚Ä¢ Mean dependencies: {dependency_values.mean():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Median dependencies: {np.median(dependency_values):.2f}\")\n",
    "        \n",
    "        # Calculate 95th percentile safely\n",
    "        if len(dependency_values) > 1:\n",
    "            percentile_95 = np.percentile(dependency_values, 95)\n",
    "            print(f\"   ‚Ä¢ 95th percentile: {percentile_95:.2f}\")\n",
    "        else:\n",
    "            print(f\"   ‚Ä¢ 95th percentile: {dependency_values[0]:.2f} (single value)\")\n",
    "            \n",
    "        print(f\"   ‚Ä¢ Max dependencies: {dependency_values.max():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Min dependencies: {dependency_values.min():.2f}\")\n",
    "        \n",
    "        # Samples with data flow\n",
    "        samples_with_flow = successful_pdg[successful_pdg['has_data_flow']]\n",
    "        data_flow_rate = len(samples_with_flow) / len(successful_pdg) * 100\n",
    "        \n",
    "        print(f\"\\nüìä DATA FLOW REQUIREMENTS:\")\n",
    "        print(f\"   ‚Ä¢ Samples with data dependencies: {len(samples_with_flow)}/{len(successful_pdg)} ({data_flow_rate:.1f}%)\")\n",
    "        \n",
    "        # CWE-specific data flow requirements\n",
    "        print(f\"\\nüîç DATA FLOW BY CWE TYPE:\")\n",
    "        for cwe in successful_pdg['cwe'].unique():\n",
    "            cwe_data = successful_pdg[successful_pdg['cwe'] == cwe]\n",
    "            cwe_flow = cwe_data[cwe_data['has_data_flow']]\n",
    "            cwe_flow_rate = len(cwe_flow) / len(cwe_data) * 100\n",
    "            avg_deps = cwe_data['dependency_count'].mean()\n",
    "            \n",
    "            print(f\"   ‚Ä¢ {cwe}: {len(cwe_flow)}/{len(cwe_data)} with flow ({cwe_flow_rate:.1f}%), avg {avg_deps:.1f} deps\")\n",
    "        \n",
    "        # Determine PDG necessity\n",
    "        dependency_threshold = 5  # Arbitrary threshold for \"significant\" dependencies\n",
    "        significant_deps = successful_pdg[successful_pdg['dependency_count'] >= dependency_threshold]\n",
    "        pdg_necessity_rate = len(significant_deps) / len(successful_pdg) * 100\n",
    "        \n",
    "        print(f\"\\nüéØ PDG NECESSITY ANALYSIS:\")\n",
    "        print(f\"   ‚Ä¢ Samples requiring PDG analysis: {len(significant_deps)}/{len(successful_pdg)} ({pdg_necessity_rate:.1f}%)\")\n",
    "        \n",
    "        if pdg_necessity_rate > 50:\n",
    "            print(\"   ‚Ä¢ RECOMMENDATION: PDG analysis is important for this dataset\")\n",
    "        elif pdg_necessity_rate > 20:\n",
    "            print(\"   ‚Ä¢ RECOMMENDATION: PDG analysis provides moderate value\")\n",
    "        else:\n",
    "            print(\"   ‚Ä¢ RECOMMENDATION: AST-only may be sufficient for most cases\")\n",
    "    \n",
    "    # Save results\n",
    "    pdg_df.to_csv(results_dir / 'data_dependency_analysis.csv', index=False)\n",
    "    print(f\"\\nüíæ Data dependency analysis saved to: {results_dir / 'data_dependency_analysis.csv'}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No data dependency analysis results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5e811",
   "metadata": {},
   "source": [
    "## 4.5 Architecture Decision Analysis\n",
    "\n",
    "Compare the effectiveness and cost of different architectural approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12372da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è COMPARING ARCHITECTURAL APPROACHES\n",
      "============================================================\n",
      "üìä ARCHITECTURE COMPARISON RESULTS:\n",
      "   ‚Ä¢ Total samples analyzed: 2413\n",
      "\n",
      "‚úÖ SUCCESS RATES:\n",
      "   ‚Ä¢ AST extraction: 100.0%\n",
      "   ‚Ä¢ CFG construction: 100.0%\n",
      "   ‚Ä¢ PDG construction: 100.0%\n",
      "   ‚Ä¢ AST vulnerability detection: 96.9%\n",
      "   ‚Ä¢ Complex control flow requiring CFG: 0.0%\n",
      "   ‚Ä¢ Significant data dependencies requiring PDG: 64.4%\n",
      "\n",
      "üéØ ARCHITECTURE RECOMMENDATIONS:\n",
      "   ‚Ä¢ RECOMMENDED ARCHITECTURE: AST + PDG\n",
      "   ‚Ä¢ REASONING: Low control flow complexity, high data dependency requirements\n",
      "\n",
      "üí∞ COST-BENEFIT ANALYSIS:\n",
      "   ‚Ä¢ AST-only processing time: 1.0ms\n",
      "   ‚Ä¢ AST+CFG processing time: 6.0ms\n",
      "   ‚Ä¢ AST+PDG processing time: 11.0ms\n",
      "   ‚Ä¢ Full AST+CFG+PDG time: 16.0ms\n",
      "   ‚Ä¢ Recommended effectiveness: 100.0%\n",
      "   ‚Ä¢ Processing efficiency: 9.1% per ms\n",
      "\n",
      "üíæ Architecture decision saved to: ../results/architecture_decision.json\n"
     ]
    }
   ],
   "source": [
    "def compare_architectural_approaches(ast_analysis, cf_analysis, pdg_analysis):\n",
    "    \"\"\"Compare different architectural approaches\"\"\"\n",
    "    print(\"üèóÔ∏è COMPARING ARCHITECTURAL APPROACHES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    ast_df = pd.DataFrame(ast_analysis) if ast_analysis else pd.DataFrame()\n",
    "    cf_df = pd.DataFrame(cf_analysis) if cf_analysis else pd.DataFrame()\n",
    "    pdg_df = pd.DataFrame(pdg_analysis) if pdg_analysis else pd.DataFrame()\n",
    "    \n",
    "    # Merge results by CVE ID\n",
    "    if not ast_df.empty and not cf_df.empty and not pdg_df.empty:\n",
    "        # Merge all analyses\n",
    "        merged_df = ast_df.merge(cf_df, on=['cwe', 'cve_id', 'lines', 'chars'], suffixes=('_ast', '_cfg'))\n",
    "        merged_df = merged_df.merge(pdg_df, on=['cwe', 'cve_id', 'lines', 'chars'], suffixes=('', '_pdg'))\n",
    "        \n",
    "        print(f\"üìä ARCHITECTURE COMPARISON RESULTS:\")\n",
    "        print(f\"   ‚Ä¢ Total samples analyzed: {len(merged_df)}\")\n",
    "        \n",
    "        # Success rates\n",
    "        ast_success = merged_df['ast_success'].sum() / len(merged_df) * 100\n",
    "        cfg_success = merged_df['cfg_success'].sum() / len(merged_df) * 100\n",
    "        pdg_success = merged_df['pdg_success'].sum() / len(merged_df) * 100\n",
    "        \n",
    "        print(f\"\\n‚úÖ SUCCESS RATES:\")\n",
    "        print(f\"   ‚Ä¢ AST extraction: {ast_success:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ CFG construction: {cfg_success:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ PDG construction: {pdg_success:.1f}%\")\n",
    "        \n",
    "        # Detection capabilities\n",
    "        if 'indicator_count' in merged_df.columns:\n",
    "            ast_detection = (merged_df['indicator_count'] > 0).sum() / len(merged_df) * 100\n",
    "            print(f\"   ‚Ä¢ AST vulnerability detection: {ast_detection:.1f}%\")\n",
    "        \n",
    "        # Complexity analysis\n",
    "        if 'cyclomatic_complexity' in merged_df.columns:\n",
    "            complex_cfg = (merged_df['cyclomatic_complexity'] > 10).sum() / len(merged_df) * 100\n",
    "            print(f\"   ‚Ä¢ Complex control flow requiring CFG: {complex_cfg:.1f}%\")\n",
    "        \n",
    "        # Data flow analysis\n",
    "        if 'dependency_count' in merged_df.columns:\n",
    "            significant_pdg = (merged_df['dependency_count'] >= 5).sum() / len(merged_df) * 100\n",
    "            print(f\"   ‚Ä¢ Significant data dependencies requiring PDG: {significant_pdg:.1f}%\")\n",
    "        \n",
    "        # Architecture recommendations\n",
    "        print(f\"\\nüéØ ARCHITECTURE RECOMMENDATIONS:\")\n",
    "        \n",
    "        # Determine optimal architecture\n",
    "        if ast_success > 95 and ast_detection > 80:\n",
    "            if complex_cfg < 30 and significant_pdg < 30:\n",
    "                recommendation = \"AST-ONLY\"\n",
    "                reasoning = \"High AST success rate and detection capability, low complexity requirements\"\n",
    "            elif complex_cfg > 50 and significant_pdg < 30:\n",
    "                recommendation = \"AST + CFG\"\n",
    "                reasoning = \"High control flow complexity, low data dependency requirements\"\n",
    "            elif complex_cfg < 30 and significant_pdg > 50:\n",
    "                recommendation = \"AST + PDG\"\n",
    "                reasoning = \"Low control flow complexity, high data dependency requirements\"\n",
    "            else:\n",
    "                recommendation = \"AST + CFG + PDG\"\n",
    "                reasoning = \"High complexity in both control flow and data dependencies\"\n",
    "        else:\n",
    "            recommendation = \"AST + CFG + PDG\"\n",
    "            reasoning = \"AST alone insufficient, full analysis recommended\"\n",
    "        \n",
    "        print(f\"   ‚Ä¢ RECOMMENDED ARCHITECTURE: {recommendation}\")\n",
    "        print(f\"   ‚Ä¢ REASONING: {reasoning}\")\n",
    "        \n",
    "        # Cost-benefit analysis\n",
    "        print(f\"\\nüí∞ COST-BENEFIT ANALYSIS:\")\n",
    "        \n",
    "        # Processing time estimates (from Phase 2)\n",
    "        ast_time = 0.001  # 1ms\n",
    "        cfg_time = 0.005  # 5ms\n",
    "        pdg_time = 0.010  # 10ms\n",
    "        \n",
    "        ast_only_cost = ast_time\n",
    "        ast_cfg_cost = ast_time + cfg_time\n",
    "        ast_pdg_cost = ast_time + pdg_time\n",
    "        full_cost = ast_time + cfg_time + pdg_time\n",
    "        \n",
    "        print(f\"   ‚Ä¢ AST-only processing time: {ast_only_cost*1000:.1f}ms\")\n",
    "        print(f\"   ‚Ä¢ AST+CFG processing time: {ast_cfg_cost*1000:.1f}ms\")\n",
    "        print(f\"   ‚Ä¢ AST+PDG processing time: {ast_pdg_cost*1000:.1f}ms\")\n",
    "        print(f\"   ‚Ä¢ Full AST+CFG+PDG time: {full_cost*1000:.1f}ms\")\n",
    "        \n",
    "        # Effectiveness vs cost\n",
    "        if recommendation == \"AST-ONLY\":\n",
    "            effectiveness = ast_detection\n",
    "            cost = ast_only_cost\n",
    "        elif recommendation == \"AST + CFG\":\n",
    "            effectiveness = min(100, ast_detection + complex_cfg * 0.5)\n",
    "            cost = ast_cfg_cost\n",
    "        elif recommendation == \"AST + PDG\":\n",
    "            effectiveness = min(100, ast_detection + significant_pdg * 0.5)\n",
    "            cost = ast_pdg_cost\n",
    "        else:\n",
    "            effectiveness = 95  # Estimated full effectiveness\n",
    "            cost = full_cost\n",
    "        \n",
    "        efficiency = effectiveness / (cost * 1000)  # Effectiveness per millisecond\n",
    "        print(f\"   ‚Ä¢ Recommended effectiveness: {effectiveness:.1f}%\")\n",
    "        print(f\"   ‚Ä¢ Processing efficiency: {efficiency:.1f}% per ms\")\n",
    "        \n",
    "        # Save architecture analysis\n",
    "        architecture_summary = {\n",
    "            \"recommendation\": recommendation,\n",
    "            \"reasoning\": reasoning,\n",
    "            \"success_rates\": {\n",
    "                \"ast\": ast_success,\n",
    "                \"cfg\": cfg_success,\n",
    "                \"pdg\": pdg_success\n",
    "            },\n",
    "            \"complexity_requirements\": {\n",
    "                \"complex_cfg_rate\": complex_cfg,\n",
    "                \"significant_pdg_rate\": significant_pdg\n",
    "            },\n",
    "            \"cost_analysis\": {\n",
    "                \"ast_only_ms\": ast_only_cost * 1000,\n",
    "                \"ast_cfg_ms\": ast_cfg_cost * 1000,\n",
    "                \"ast_pdg_ms\": ast_pdg_cost * 1000,\n",
    "                \"full_ms\": full_cost * 1000\n",
    "            },\n",
    "            \"effectiveness\": effectiveness,\n",
    "            \"efficiency\": efficiency\n",
    "        }\n",
    "        \n",
    "        with open(results_dir / 'architecture_decision.json', 'w') as f:\n",
    "            json.dump(architecture_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüíæ Architecture decision saved to: {results_dir / 'architecture_decision.json'}\")\n",
    "        \n",
    "        return architecture_summary\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Insufficient data for architecture comparison\")\n",
    "        return None\n",
    "\n",
    "# Compare architectural approaches\n",
    "if 'ast_analysis' in locals() and 'cf_analysis' in locals() and 'pdg_analysis' in locals():\n",
    "    architecture_decision = compare_architectural_approaches(ast_analysis, cf_analysis, pdg_analysis)\n",
    "else:\n",
    "    print(\"‚ùå Missing analysis data for architecture comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b4911",
   "metadata": {},
   "source": [
    "## 4.6 Visualization and Final Recommendations\n",
    "\n",
    "Create visualizations to support the architecture decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3d1c1",
   "metadata": {},
   "source": [
    "##  Phase 4 Completion Summary\n",
    "\n",
    "Final architecture decision and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ce1e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéØ PHASE 4: STRUCTURAL ANALYSIS NECESSITY - COMPLETION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "‚úÖ 4.1 VULNERABILITY SAMPLE ANALYSIS COMPLETED:\n",
      "   ‚Ä¢ 2317 vulnerability samples analyzed\n",
      "   ‚Ä¢ 10 CWE types covered\n",
      "\n",
      "‚úÖ 4.2 AST-ONLY EFFECTIVENESS ANALYZED:\n",
      "   ‚Ä¢ AST extraction success: 2317/2317 (100.0%)\n",
      "   ‚Ä¢ Vulnerability detection rate: 97.1%\n",
      "\n",
      "‚úÖ 4.3 CONTROL FLOW COMPLEXITY ANALYZED:\n",
      "   ‚Ä¢ CFG construction success: 2317/2317 (100.0%)\n",
      "   ‚Ä¢ Complex control flow rate: 0.0%\n",
      "\n",
      "‚úÖ 4.4 DATA DEPENDENCY REQUIREMENTS ANALYZED:\n",
      "   ‚Ä¢ PDG construction success: 2317/2317 (100.0%)\n",
      "   ‚Ä¢ Significant data dependencies: 66.2%\n",
      "\n",
      "‚úÖ 4.5 ARCHITECTURE DECISION MADE:\n",
      "   ‚Ä¢ Recommended architecture: AST + PDG\n",
      "   ‚Ä¢ Expected effectiveness: 100.0%\n",
      "   ‚Ä¢ Processing efficiency: 9.1% per ms\n",
      "\n",
      "üî¨ KEY ARCHITECTURE FINDINGS:\n",
      "   1. AST-only detection is effective (97.1% success rate)\n",
      "   2. Low control flow complexity - CFG may be optional (0.0% complex)\n",
      "   3. High data dependency requirements need PDG analysis (66.2% significant)\n",
      "   4. Optimal architecture determined: AST + PDG\n",
      "\n",
      "üìÅ GENERATED FILES:\n",
      "   ‚úÖ ast_only_analysis.csv (191.1 KB)\n",
      "   ‚úÖ control_flow_analysis.csv (94.5 KB)\n",
      "   ‚úÖ data_dependency_analysis.csv (97.4 KB)\n",
      "   ‚úÖ architecture_decision.json (0.5 KB)\n",
      "\n",
      " VALIDATION OF ARCHITECTURE DECISIONS:\n",
      "   ‚Ä¢ ‚úÖ PDG analysis adds value to vulnerability detection\n",
      "   ‚Ä¢ ‚úÖ Sufficient data for architecture analysis\n",
      "   ‚Ä¢ ‚úÖ Recommended architecture achieves high effectiveness\n",
      "\n",
      "üöÄ NEXT STEPS:\n",
      "   1. Implement recommended architecture in production\n",
      "   2. Validate architecture performance on full dataset\n",
      "   3. Proceed to Phase 5: Configuration Derivation\n",
      "   4. Update project configuration with architecture decision\n",
      "\n",
      " PHASE 4 COMPLETE - ARCHITECTURE DECISION MADE!\n",
      "   AST effectiveness: ANALYZED ‚úÖ\n",
      "   CFG necessity: EVALUATED ‚úÖ\n",
      "   PDG requirements: ASSESSED ‚úÖ\n",
      "   Optimal architecture: DETERMINED ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "# Phase 4 completion summary\n",
    "print(\"=\"*80)\n",
    "print(\"üéØ PHASE 4: STRUCTURAL ANALYSIS NECESSITY - COMPLETION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summarize what was accomplished\n",
    "if 'analysis_samples' in locals():\n",
    "    print(f\"\\n‚úÖ 4.1 VULNERABILITY SAMPLE ANALYSIS COMPLETED:\")\n",
    "    print(f\"   ‚Ä¢ {len(analysis_samples)} vulnerability samples analyzed\")\n",
    "    cwe_counts = Counter(sample['cwe'] for sample in analysis_samples)\n",
    "    print(f\"   ‚Ä¢ {len(cwe_counts)} CWE types covered\")\n",
    "\n",
    "if 'ast_analysis' in locals() and ast_analysis:\n",
    "    ast_df = pd.DataFrame(ast_analysis)\n",
    "    successful_ast = ast_df[ast_df['ast_success']]\n",
    "    print(f\"\\n‚úÖ 4.2 AST-ONLY EFFECTIVENESS ANALYZED:\")\n",
    "    print(f\"   ‚Ä¢ AST extraction success: {len(successful_ast)}/{len(ast_df)} ({len(successful_ast)/len(ast_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_ast) > 0:\n",
    "        detection_rate = (successful_ast['indicator_count'] > 0).mean() * 100\n",
    "        print(f\"   ‚Ä¢ Vulnerability detection rate: {detection_rate:.1f}%\")\n",
    "\n",
    "if 'cf_analysis' in locals() and cf_analysis:\n",
    "    cf_df = pd.DataFrame(cf_analysis)\n",
    "    successful_cfg = cf_df[cf_df['cfg_success']]\n",
    "    print(f\"\\n‚úÖ 4.3 CONTROL FLOW COMPLEXITY ANALYZED:\")\n",
    "    print(f\"   ‚Ä¢ CFG construction success: {len(successful_cfg)}/{len(cf_df)} ({len(successful_cfg)/len(cf_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_cfg) > 0:\n",
    "        complex_rate = (successful_cfg['cyclomatic_complexity'] > 10).mean() * 100\n",
    "        print(f\"   ‚Ä¢ Complex control flow rate: {complex_rate:.1f}%\")\n",
    "\n",
    "if 'pdg_analysis' in locals() and pdg_analysis:\n",
    "    pdg_df = pd.DataFrame(pdg_analysis)\n",
    "    successful_pdg = pdg_df[pdg_df['pdg_success']]\n",
    "    print(f\"\\n‚úÖ 4.4 DATA DEPENDENCY REQUIREMENTS ANALYZED:\")\n",
    "    print(f\"   ‚Ä¢ PDG construction success: {len(successful_pdg)}/{len(pdg_df)} ({len(successful_pdg)/len(pdg_df)*100:.1f}%)\")\n",
    "    \n",
    "    if len(successful_pdg) > 0:\n",
    "        significant_rate = (successful_pdg['dependency_count'] >= 5).mean() * 100\n",
    "        print(f\"   ‚Ä¢ Significant data dependencies: {significant_rate:.1f}%\")\n",
    "\n",
    "if 'architecture_decision' in locals() and architecture_decision:\n",
    "    print(f\"\\n‚úÖ 4.5 ARCHITECTURE DECISION MADE:\")\n",
    "    rec = architecture_decision['recommendation']\n",
    "    effectiveness = architecture_decision['effectiveness']\n",
    "    print(f\"   ‚Ä¢ Recommended architecture: {rec}\")\n",
    "    print(f\"   ‚Ä¢ Expected effectiveness: {effectiveness:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Processing efficiency: {architecture_decision['efficiency']:.1f}% per ms\")\n",
    "\n",
    "# Key findings\n",
    "print(f\"\\nüî¨ KEY ARCHITECTURE FINDINGS:\")\n",
    "\n",
    "findings = []\n",
    "\n",
    "if 'ast_analysis' in locals() and ast_analysis:\n",
    "    ast_df = pd.DataFrame(ast_analysis)\n",
    "    successful_ast = ast_df[ast_df['ast_success']]\n",
    "    if len(successful_ast) > 0:\n",
    "        detection_rate = (successful_ast['indicator_count'] > 0).mean() * 100\n",
    "        if detection_rate > 80:\n",
    "            findings.append(f\"AST-only detection is effective ({detection_rate:.1f}% success rate)\")\n",
    "        else:\n",
    "            findings.append(f\"AST-only detection has limitations ({detection_rate:.1f}% success rate)\")\n",
    "\n",
    "if 'cf_analysis' in locals() and cf_analysis:\n",
    "    cf_df = pd.DataFrame(cf_analysis)\n",
    "    successful_cfg = cf_df[cf_df['cfg_success']]\n",
    "    if len(successful_cfg) > 0:\n",
    "        complex_rate = (successful_cfg['cyclomatic_complexity'] > 10).mean() * 100\n",
    "        if complex_rate > 50:\n",
    "            findings.append(f\"High control flow complexity requires CFG analysis ({complex_rate:.1f}% complex)\")\n",
    "        else:\n",
    "            findings.append(f\"Low control flow complexity - CFG may be optional ({complex_rate:.1f}% complex)\")\n",
    "\n",
    "if 'pdg_analysis' in locals() and pdg_analysis:\n",
    "    pdg_df = pd.DataFrame(pdg_analysis)\n",
    "    successful_pdg = pdg_df[pdg_df['pdg_success']]\n",
    "    if len(successful_pdg) > 0:\n",
    "        significant_rate = (successful_pdg['dependency_count'] >= 5).mean() * 100\n",
    "        if significant_rate > 50:\n",
    "            findings.append(f\"High data dependency requirements need PDG analysis ({significant_rate:.1f}% significant)\")\n",
    "        else:\n",
    "            findings.append(f\"Low data dependency requirements - PDG may be optional ({significant_rate:.1f}% significant)\")\n",
    "\n",
    "if 'architecture_decision' in locals() and architecture_decision:\n",
    "    rec = architecture_decision['recommendation']\n",
    "    findings.append(f\"Optimal architecture determined: {rec}\")\n",
    "\n",
    "for i, finding in enumerate(findings, 1):\n",
    "    print(f\"   {i}. {finding}\")\n",
    "\n",
    "# Files generated\n",
    "print(f\"\\nüìÅ GENERATED FILES:\")\n",
    "output_files = [\n",
    "    'ast_only_analysis.csv',\n",
    "    'control_flow_analysis.csv', \n",
    "    'data_dependency_analysis.csv',\n",
    "    'architecture_decision.json'\n",
    "]\n",
    "\n",
    "for file_name in output_files:\n",
    "    file_path = results_dir / file_name\n",
    "    if file_path.exists():\n",
    "        size_kb = file_path.stat().st_size / 1024\n",
    "        print(f\"   ‚úÖ {file_name} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file_name} (not generated)\")\n",
    "\n",
    "# Validation of architecture decisions\n",
    "print(f\"\\n VALIDATION OF ARCHITECTURE DECISIONS:\")\n",
    "\n",
    "validations = []\n",
    "\n",
    "if 'architecture_decision' in locals() and architecture_decision:\n",
    "    rec = architecture_decision['recommendation']\n",
    "    if rec == \"AST-ONLY\":\n",
    "        validations.append(\"‚úÖ AST-only architecture sufficient for this dataset\")\n",
    "    elif \"CFG\" in rec:\n",
    "        validations.append(\"‚úÖ CFG analysis adds value to vulnerability detection\")\n",
    "    elif \"PDG\" in rec:\n",
    "        validations.append(\"‚úÖ PDG analysis adds value to vulnerability detection\")\n",
    "    else:\n",
    "        validations.append(\"‚úÖ Full analysis pipeline recommended\")\n",
    "\n",
    "if 'ast_analysis' in locals() and ast_analysis:\n",
    "    ast_df = pd.DataFrame(ast_analysis)\n",
    "    if len(ast_df) > 50:\n",
    "        validations.append(\"‚úÖ Sufficient data for architecture analysis\")\n",
    "    else:\n",
    "        validations.append(\"‚ö†Ô∏è Limited data - results should be interpreted cautiously\")\n",
    "\n",
    "if 'architecture_decision' in locals() and architecture_decision:\n",
    "    effectiveness = architecture_decision['effectiveness']\n",
    "    if effectiveness > 80:\n",
    "        validations.append(\"‚úÖ Recommended architecture achieves high effectiveness\")\n",
    "    else:\n",
    "        validations.append(\"‚ö†Ô∏è Recommended architecture has effectiveness limitations\")\n",
    "\n",
    "for validation in validations:\n",
    "    print(f\"   ‚Ä¢ {validation}\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   1. Implement recommended architecture in production\")\n",
    "print(f\"   2. Validate architecture performance on full dataset\")\n",
    "print(f\"   3. Proceed to Phase 5: Configuration Derivation\")\n",
    "print(f\"   4. Update project configuration with architecture decision\")\n",
    "\n",
    "print(f\"\\n PHASE 4 COMPLETE - ARCHITECTURE DECISION MADE!\")\n",
    "print(f\"   AST effectiveness: ANALYZED ‚úÖ\")\n",
    "print(f\"   CFG necessity: EVALUATED ‚úÖ\")\n",
    "print(f\"   PDG requirements: ASSESSED ‚úÖ\")\n",
    "print(f\"   Optimal architecture: DETERMINED ‚úÖ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
